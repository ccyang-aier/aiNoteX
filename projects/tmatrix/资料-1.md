## 稀疏化框架

关于：推理引擎集成稀疏注意力框架、稀疏KVCache框架，当前社区存在类似RFC，但仍处于探讨阶段，可研究通过压缩、聚类、驱逐、选择、低秩、共享、重算等策略（相关论文很多）去实现稀疏注意力以及稀疏KVCache框架并集成到如vLLM推理引擎中，实现降低显存占用、计算复杂度以及降低显存带宽的效果（尤其在长上下文场景下）；

关于在vllm中引入稀疏KVCache管理框架，以支持在长上下文场景下显著降低TTFT，提升推理速度。

相关RFC：

1. [https://github.com/vllm-project/vllm/issues/5751](https://github.com/vllm-project/vllm/issues/5751) ：最早的提案RFC，提出将H2O、SnapKV等稀疏KV算法集成到vllm，但后续一直处于开发状态，未有结果，后续vllm主线路线图中陆续两个季度提出要支持，但一直未关闭；
2. [https://github.com/vllm-project/vllm/issues/12254](https://github.com/vllm-project/vllm/issues/12254)
3. [https://github.com/vllm-project/vllm/issues/21772](https://github.com/vllm-project/vllm/issues/21772)
4. [https://github.com/vllm-project/vllm/pull/11928](https://github.com/vllm-project/vllm/pull/11928)

21772，最近又被社区提出关于引入稀疏KVCache管理框架的RFC，架构如下：

![](chrome-extension://difoiogjjojoaoomphldepapgpbgkhkb/imgs/Pasted%20image%2020250822162849.png)

相关较早提出该思想的paper较多，如H2O，SnapKV等。
[https://arxiv.org/abs/2306.14048](https://arxiv.org/abs/2306.14048) 
[https://arxiv.org/abs/2404.14469](https://arxiv.org/abs/2404.14469) 
[https://arxiv.org/html/2406.02069](https://arxiv.org/html/2406.02069) 
[https://arxiv.org/html/2412.03131](https://arxiv.org/html/2412.03131)

这里实现有两个关键点：一个是整个框架的设计与实现，一个是关于稀疏KV策略的调优与选择，要能识别出不同的paper提出的算法的优劣和适用面，并选择性的挑选最优的进行引入；

稀疏注意力社区也有讨论，相关paper如之前分析的ShadowKV等，本质上也属于稀疏注意力方向上的设计，只不过这些都需要一个架子或者插件去接入到推理引擎侧，类似于稀疏KVCache；