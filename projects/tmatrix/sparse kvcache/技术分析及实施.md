## 1.背景

长文本推理场景下，KVCache占用会进一步激增，这导致了Decode阶段的Memory-Bound现象被加剧，从而造成解码过程被拖慢，同时，这也等同于变相延长了KVCache的生命周期，导致Prefill阶段被影响，最终使得整体系统的吞吐变低，并发也上不去。

为了解决这种问题，KVCache的稀疏性是一个近两年来都不断被关注的重点方向。

它的核心思想在于：**LLM的推理过程中，Attention Score主要由少部分重要的Token得到，而其余大部分Token对结果的影响很小，即呈现出明显的稀疏性。**

在总结了近两年绝大部分经典、优秀的KVCache稀疏性研究论文之后，我认为，一个完整的KVCache稀疏算法，在设计时应当至少关注以下几个方面：

- **Top-K Token怎么选？**
- **如何考虑不同层之间的稀疏性？**
- **KV Cache的处理策略？是驱逐还是压缩？还是不处理？**
- ......

接下来将首先讨论最近2-3年的数十篇经典、优秀的有关KVCache稀疏性研究相关的paper，然后结合自己的分析以及vLLM等社区当前最新的发展情况，给出一个落地KVCache稀疏特性到生产环境的架构设计和实施方向。

## 2.近三年论文

静态KVCache稀疏算法

早期的静态稀疏方法Slide window，StreamLLM这些弊端比较明显，目前主要研究方向都是动态稀疏。

动态KVCache稀疏算法

### H2O
论文地址：[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/html/2306.14048v3)

#### 设计思想与实现

背景：
- 自回归生成（decoding）过程中，每步Token生成都需要对历史所有token的KV缓存做注意力计算；
- 长上下文/长生成导致 KV cache 内存线性增长，显存和带宽成为瓶颈；
- 传统策略：只保留“最近窗口”或“固定稀疏采样”，但可能丢失对Token生成至关重要的远距离依赖；

洞察：
- 模型在生成过程中，注意力块中对过往Token的累计注意力分数呈明显**幂律分布**，存在数量很少但贡献极大的token（Heavy Hitters，简称 $H_2$）
- 只要尽可能保留这小撮真正“有用”的Token，再适度保留一部分“最近”的Token，就能在强力压缩KV cache的同时，几乎不损伤模型生成能力，即兼顾**长短距离依赖**；

设计思路：
1. 在线推理过程中，为每个历史Token维护“累计注意力分数”，即在每步生成新Token时，对历史Token累加新Token对其的注意力分数；
2. 将选择Top-K Token的过程抽象为贪心问题的最优解，即：在有限缓存容量下，优先保留累计注意力最高的一小部分Token（长距离依赖），并预留一部分最近窗口内的Token（短距离依赖）；
3. 注意：要理解累计注意力分数是一个“历史累计重要性”指标，反映的是“这个Token在后续生成中被关注了多少次、累计有多重要”，不是一时一刻的瞬时值，这些累计分数往往呈幂律或高度不均匀分布——少数Token的累计值远高于大多数 token，这个现象正是$H_2O$算法的设计基础；

算法大致执行过程：
1. 维护一个分数表：每步从注意力矩阵取到对各历史Token的权重并累加，该过程是对现有注意力结果的轻量读数，不引入额外前向计算；
2. 选择阶段：
	- 固定容量：$K$，固定窗口: $R$；
	- 先固定保留最近的 $R$ 个位置以保证短期连贯性；
	- 在剩余预算 $K−R$ 内，再从历史Token中选出累计注意力分数最高的$H_2  \;\; Token$；
3. 写回与淘汰：
    - 新生成Token的KV直接写入；
    - 避免swap，按evict_ids批量覆盖被淘汰位置，避免昂贵的内存搬移与碎片化；

数学式：
累计注意力分数定义：$$\text{AccumScore}(j) = \sum_{i=j+1}^{n} \sum_{l=1}^{L} \sum_{h=1}^{H} \text{Attention}_{i,l,h}[j]$$
其中：
- $j$：历史token位置
- $i$：当前时间步
- $l$：层索引
- $h$：注意力头索引

定义一个Token为Heavy Hitter当且仅当： $$\text{AccumScore}(j) \geq \theta \cdot \max_{k \leq i} \text{AccumScore}(k)$$其中$\theta$是阈值参数，论文中通常取20%左右，$\max_{k \leq i} \text{AccumScore}(k)$是当前时刻$i$为止的“累计分数最大值”，设$θ=0.2$，则只有那些累计分数达到“当前累计分数最大值的20%以上”的Token才被标记为$H_2  \;\; Token$，因为AccumScore的分布呈幂律分布，通常“最大”会远大于大多数Token的累计分数，所以满足$\max_{k \leq i} \text{AccumScore}(k)$的20%的Token数量并不会很多；

![500](imgs/Pasted%20image%2020250830232503.png)
如图，设$K = 3$，第四步解码完成后，与第三个Token相关的KV将根据累积注意力得分被驱逐，因此，这些被驱逐的KV在后续解码步骤中将无法访问，第五步解码过程同理。

同时，可以发现，前N个Token更有可能成为累计注意力分数最高的Token，这不仅仅是因为它们作为最早一批历史Token，被累加了更多次注意力得分，也与某些论文提出的Attention Sink结论不谋而合，即在一次推理过程中，注意力更有可能集中于前几个Token。

![500](imgs/Pasted%20image%2020250831004332.png)

#### 总结与拓展
1. 在$Top-K$算法的设计上，$H_2O$ 提出基于累计注意力分数去筛选高重要性Token，同时兼顾短距离依赖选择近期Token，整体设计思路清晰，兼顾长短距离依赖，同时将整个筛选过程抽象为一个贪心算法的求解过程，基于固定KVCache余量$K$和固定窗口额度$R$，算法复杂度低，工程化成本低，易于实现；
2. 基于贪心策略挑战Top-K，容易陷入局部最优问题，比如曾经被视为不重要的Token在后续变得重要，但已经被剔除，从而影响生成质量；
3. 未关注模型不同层的差异化稀疏策略；

集成到vLLM等生产级推理引擎：

vLLM要融合$H₂O$，需要解决两层适配：
- 机制层：PageAttention的页式分配、回收与索引；
- 内核层：FlashAttention等算子如何只在“保留集合”上做注意力并暴露必要统计；


### SnapKV
论文地址：[SnapKV: LLM Knows What You are Looking for Before Generation](https://arxiv.org/html/2404.14469v2)

#### 设计思想与实现
背景：
1. 很多长上下文场景实际上Prompt Token是性能瓶颈，而现有KV压缩/驱逐方法多聚焦生成阶段新增Token的KV压缩，忽视了Prompt Token的KV压缩这一实际应用中的主瓶颈；
2. 如$H₂O$算法，虽然能以简单清晰的贪婪策略筛选重要Token，但论文没有过多评估对Prompt Token的适应性，仅提到当输入Token超出2K时，可沿用该算法压缩输入Token；

洞察：
1. 将提示按窗口（128-token）划分，并用最后一个提示窗口（观测窗口）计算其对前缀各位置的平均注意力，与真实生成阶段的注意力“重要位置”进行重叠率对比，发现最后窗口的选择与生成时实际用到的特征高度一致，这意味着：**LLM在开始生成前，就已经知道它会关注Prompt Token里的哪些部分**；
2. 将生成部分分为多个窗口，逐段与提示最后窗口所选的重要位置比较，层间重叠率保持较高，表明这种分配模式在生成阶段相当稳定，这意味着：注意力重要位置在生成过程中保持稳定，生成过程不会频繁改变要看的Prompt Token位置，因此可以在生成开始前选定并压缩这些位置；

此外，作者还验证：
- 指令内容会影响“重要前缀位置”的选择（问题改变→选择变化），因此压缩策略必须上下文自适应，不能依赖静态权重或固定策略；
- 指令在提示中的位置（前/后）不会显著影响上述可预判性与稳定性；

设计思路：
目标：将Prompt Token KV压缩为常量规模，打破解码时延随输入长度线性增长的模式，并大幅节省显存；

设提示长度：$L_{prompt}$​，划分得到，前缀：$L_{prefix}$​，观测窗口（一般是输入Token的最后一段，用来投票识别输入中的重要Token位置）：$L_{obs}$​，满足$L_{prompt} = L_{prefix} + L_{obs}$；

投票（Voting）：在观测窗口内，汇聚各个query（观测窗口内每个Token的query）对输入各位置的注意力（对所有头累加），得到对前缀位置的重要性打分；

压缩率参数 $p$：保留的前缀位置数：$k=⌊p×L_{prefix}⌋$

总体流程：
1. 基于观测窗口进行投票，选择重要位置
    - 计算观测窗口内所有query对前缀各位置的$softmax$注意力权重，并按头聚合求和，得到输入位置的重要性矩阵$C$；
    - 对每个头独立地选$Top-k$的前缀位置索引$I=Topk(C,k)$；
2. 拼接压缩后的 KV
    - 从原始KV中，按索引提取被选中的前缀KV，并与观测窗口的KV全量拼接，作为新的提示端KV缓存，生成阶段仅访问这段常量规模的提示 KV即可；

补充：高效“聚类”（Pooling）
- 仅保留Top-k散点会丢失上下文连续性，影响拷贝/引用能力（如电话号码只留区号，导致后续幻觉）；
- 因而引入一维池化（max/avg pooling，核大小可调，通常5/7/13等）在投票分数上做平滑/扩张，等价于保留高分段及其邻域簇，以保住关键片段的上下文完整性与可复制性；

#### 总结与拓展

## 3.社区现状及发展

## 4.实施方向与设计