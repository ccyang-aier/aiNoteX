## 1.背景

长文本推理场景下，KVCache占用会进一步激增，这导致了Decode阶段的Memory-Bound现象被加剧，从而造成解码过程被拖慢，同时，这也等同于变相延长了KVCache的生命周期，导致Prefill阶段可能无法支撑更大的并发批次，最终使得整体系统的吞吐也随之变低。

为了提升超长上下文下的推理体验，降低推理代价和成本，围绕KVCache优化方向的研究随之激增，其中，KVCache的稀疏性是一个近两年来都不断被关注的重点方向。

它的核心思想在于：**LLM的推理过程中，Attention Score主要由少部分重要的Token得到，而其余大部分Token对结果的影响很小，即呈现出明显的稀疏性。**

另一个可以佐证的例子是大语言模型LLM的一个关键特性的研究：Massive Activations，即：在LLM中有很少数的激活值明显活跃于其他激活值，有时候甚至高于其他激活100,000倍以上，换而言之，这也可以说明少部分的Token起到了至关重要的作用，因而可以通过KV稀疏方法（即保留重要的token）来提升推理性能。

Massive Activations现象由一篇paper提出：[Massive Activations in Large Language Models](https://arxiv.org/html/2402.17762v2)

在总结了近两年绝大部分经典、优秀的KVCache稀疏性研究论文之后，我认为，一个完整的KVCache稀疏算法，在设计时应当至少关注以下几个方面：

- **Top-K Token怎么选？**
- **如何考虑跨层稀疏性？**
- **KV Cache的处理策略？是驱逐还是压缩？是否支持可召回？**
- ......

接下来将首先讨论最近2-3年的数十篇经典、优秀的有关KVCache稀疏性研究相关的paper，然后结合自己的分析以及vLLM等社区当前最新的发展情况，给出一个落地KVCache稀疏特性到生产环境的架构设计和实施方向。

## 2.稀疏KVCache论文

静态KVCache稀疏算法

早期的静态稀疏方法Slide window，StreamLLM这些弊端比较明显，目前主要研究方向都是动态稀疏。

动态KVCache稀疏算法

### Longformer

论文地址：[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)
代码仓：[https://github.com/allenai/longformer](https://github.com/allenai/longformer)

较早的涉及稀疏KVCache的论文，提出滑窗注意力，即：仅在最新Token的KVCache状态上保持一个固定大小的滑动窗口，窗口大小为w，则每个Token只存储其周围±w/2位置的KV。

![950](imgs/Pasted%20image%2020250901222020.png)

论文提出了三种滑窗注意力及其变体：
1. **滑动窗口注意力（Sliding Window Attention）**：每个Token只关注其前后w个Token（局部窗口）；
2. **膨胀滑动窗口（Dilated Sliding Window）**：在窗口内每隔d个Token关注一次，扩大整体感受野；
3. **全局注意力（Global Attention）**：少量关键Token可访问全部Token，同时也被全部Token访问，用于汇聚全局信息；

滑窗注意力确保了KV缓存初始填充后保持恒定的内存使用和解码速度，但一旦序列长度超过缓存大小，即使只是逐出第一个Token的KV，模型质量也会迅速崩溃。


### StreamingLLM

论文地址：[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/html/2309.17453)
代码仓：[https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

**背景：**
1. 推理时的KV cache随生成长度线性增长，导致显存占用大、解码时延增加；
2. 窗口注意力（window attention）：仅缓存最近L个Token的 KV，优点是显存/时延可控，缺点是当总长度超过缓存窗口、且最早的Token被逐出时，模型性能骤降；
3. 滑动窗口重计算（sliding window with recomputation）：每步对最近L的上下文重新前向构建KV，性能稳定但计算复杂度高：$O(T·L^2)$

**洞察：**

关键发现：自回归LLM的注意力在较高层/头会系统性地对序列初始Token分配极高的注意力权重，即便它们语义上不重要。

这是由于：注意力计算中的Softmax要求所有Token的注意力权重之和为1，在自回归训练中，初始Token对所有后续Token都是可见的，因此模型学会了将它们当作一个类似蓄水池的东西，当没有特别相关的历史Token时，就将多余的注意力流向这里，以满足权重归一化的要求。

**设计思路：**

结合窗口注意力的设计思想，若在窗口注意力中保留极少数最初始的Attention Sink Token的KV（例如只保留前4个Token的KV），再配合最近窗口的KV，模型在超长序列上不再崩塌，性能接近全量上下文，但计算与显存代价接近窗口注意力。

在稀疏KVCache的设计上，会维持两类KVCache：
1. Attention Sink Token的KVCache：固定保留开头极少数Token(如只保留开头4个Token)的KV作为注意力锚点；
2. Sliding Window KVCache：保留最近L个Token的KV；

在注意力计算时，新的query仅关注sink KV + 最近窗口KV即可，从而达到：
1. 显存占用与解码时延近似常数（在窗口填满后）；
2. 注意力分布更稳定；

![950](imgs/Pasted%20image%2020250901222309.png)

基于StreamingLLM的Token解码生成阶段的KVCache滚动更新策略：
![800](imgs/Pasted%20image%2020250901222705.png)

**总结与拓展**

缺陷：
1. StreamingLLM相当于一种静态稀疏KVCache，基于一种固定策略选择Token，复杂场景难以保证精度；

### H2O
论文地址：[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/html/2306.14048v3)
代码仓：[https://github.com/FMInference/H2O](https://github.com/FMInference/H2O)

**背景：**
- 自回归生成（decoding）过程中，每步Token生成都需要对历史所有token的KV缓存做注意力计算；
- 长上下文/长生成导致 KV cache 内存线性增长，显存和带宽成为瓶颈；
- 传统策略：只保留“最近窗口”或“固定稀疏采样”，但可能丢失对Token生成至关重要的远距离依赖；

**洞察：**
- 模型在生成过程中，注意力块中对过往Token的累计注意力分数呈明显**幂律分布**，存在数量很少但贡献极大的token（Heavy Hitters，简称 $H_2$）
- 只要尽可能保留这小撮真正“有用”的Token，再适度保留一部分“最近”的Token，就能在强力压缩KV cache的同时，几乎不损伤模型生成能力，即兼顾**长短距离依赖**；

**设计思路：**
1. 在线推理过程中，为每个历史Token维护“累计注意力分数”，即在每步生成新Token时，对历史Token累加新Token对其的注意力分数；
2. 将选择Top-K Token的过程抽象为贪心问题的最优解，即：在有限缓存容量下，优先保留累计注意力最高的一小部分Token（长距离依赖），并预留一部分最近窗口内的Token（短距离依赖）；
3. 注意：要理解累计注意力分数是一个“历史累计重要性”指标，反映的是“这个Token在后续生成中被关注了多少次、累计有多重要”，不是一时一刻的瞬时值，这些累计分数往往呈幂律或高度不均匀分布——少数Token的累计值远高于大多数 token，这个现象正是$H_2O$算法的设计基础；

算法大致执行过程：
1. 维护一个分数表：每步从注意力矩阵取到对各历史Token的权重并累加，该过程是对现有注意力结果的轻量读数，不引入额外前向计算；
2. 选择阶段：
	- 固定容量：$K$，固定窗口: $R$；
	- 先固定保留最近的 $R$ 个位置以保证短期连贯性；
	- 在剩余预算 $K−R$ 内，再从历史Token中选出累计注意力分数最高的$H_2  \;\; Token$；
3. 写回与淘汰：
    - 新生成Token的KV直接写入；
    - 避免swap，按evict_ids批量覆盖被淘汰位置，避免昂贵的内存搬移与碎片化；

数学式：
累计注意力分数定义：$$\text{AccumScore}(j) = \sum_{i=j+1}^{n} \sum_{l=1}^{L} \sum_{h=1}^{H} \text{Attention}_{i,l,h}[j]$$
其中：
- $j$：历史token位置
- $i$：当前时间步
- $l$：层索引
- $h$：注意力头索引

定义一个Token为Heavy Hitter当且仅当： $$\text{AccumScore}(j) \geq \theta \cdot \max_{k \leq i} \text{AccumScore}(k)$$其中$\theta$是阈值参数，论文中通常取20%左右，$\max_{k \leq i} \text{AccumScore}(k)$是当前时刻$i$为止的“累计分数最大值”，设$θ=0.2$，则只有那些累计分数达到“当前累计分数最大值的20%以上”的Token才被标记为$H_2  \;\; Token$，因为AccumScore的分布呈幂律分布，通常“最大”会远大于大多数Token的累计分数，所以满足$\max_{k \leq i} \text{AccumScore}(k)$的20%的Token数量并不会很多；

![500](imgs/Pasted%20image%2020250830232503.png)
如图，设$K = 3$，第四步解码完成后，与第三个Token相关的KV将根据累积注意力得分被驱逐，因此，这些被驱逐的KV在后续解码步骤中将无法访问，第五步解码过程同理。

同时，可以发现，前N个Token更有可能成为累计注意力分数最高的Token，这不仅仅是因为它们作为最早一批历史Token，被累加了更多次注意力得分，也与某些论文提出的Attention Sink结论不谋而合，即在一次推理过程中，注意力更有可能集中于前几个Token。

![500](imgs/Pasted%20image%2020250831004332.png)

**总结与拓展**
1. 在$Top-K$算法的设计上，$H_2O$ 提出基于累计注意力分数去筛选高重要性Token，同时兼顾短距离依赖选择近期Token，整体设计思路清晰，兼顾长短距离依赖，同时将整个筛选过程抽象为一个贪心算法的求解过程，基于固定KVCache余量$K$和固定窗口额度$R$，算法复杂度低，工程化成本低，易于实现；
2. 基于贪心策略挑战Top-K，容易陷入局部最优问题，比如曾经被视为不重要的Token在后续变得重要，但已经被剔除，从而影响生成质量；
3. 未关注模型不同层的差异化稀疏策略；

集成到vLLM等生产级推理引擎：

vLLM要融合$H₂O$，需要解决两层适配：
- 机制层：PageAttention的页式分配、回收与索引；
- 内核层：FlashAttention等算子如何只在“保留集合”上做注意力并暴露必要统计；


### FastGen
论文地址：[Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://arxiv.org/html/2310.01801)

**洞察：**
1. 注意力头之间存在稳定且可识别的结构性差异：**并非所有注意力头都广泛关注所有历史Token**；
2. 如果对不同结构的注意力头采用针对性的缓存压缩策略，可以大幅减少KV Cache占用，同时几乎不牺牲质量；

论文识别出不同的注意力头关注模式：
1. 局部型（Local）：重视近邻上下文 → 基于滑窗管理KVCache，移除远距离KV；
2. 特殊符号/标点集中型（Special/Punctuation-centric）：关注分隔符、换行、标点等 → 只保留这些特殊Token的KV；
3. 稀疏型（Column-sparse）：整体注意力在少数列上集中 → 丢弃被累计关注频率最低的一批Token（类似H2O）；
4. 广域型（Global/Uniform-ish）：关注广泛历史Token → 使用标准KV Cache，不压缩；
5. 组合/混合型：在不同阶段或不同层表现出组合特性 → 按剖析结果选择最贴合的单一策略（或分段策略）；

![950](imgs/Pasted%20image%2020250901222145.png)

**设计思路：**
1. 在Prefill阶段，先进行一次轻量的Profiling操作，它会计算出每个注意力头的注意力图（Attention Map），用于识别不同注意力头的关注模式，然后给每个头打上策略标签，用于驱动KV缓存的差异化压缩；
2. 在Decode阶段，根据为每个头选定的策略来动态管理KVCache，例如，对于被识别为“局部性”的头，只保留最近的KV向量，对于特殊Token头，则只保留特殊Token的KV向量；

**总结与拓展**

评估效果(在LLama-30B)：
1. 在压缩比例35%时，能够恢复95%+的注意力评分（attention score 近似度高）
2. 在50%缓存压缩情况下，性能优于固定策略的方法在15%压缩的表现；

优势：
1. 关注不同注意力头进行差异化的KVCache细粒度管理，压缩精度更高，质量更稳定；

缺陷：
1. 结合GQA、MQA后，压缩策略将收敛，可能会退化为单一策略，如退化成类似H2O这样的KV稀疏化策略；
2. Profiling需要识别多个不同模式，导致耗时代价过大，可能需要针对性的调优；
3. 


### SnapKV
论文地址：[SnapKV: LLM Knows What You are Looking for Before Generation](https://arxiv.org/html/2404.14469v2)

**背景：**
1. 很多长上下文场景实际上Prompt Token是性能瓶颈，而现有KV压缩/驱逐方法多聚焦生成阶段新增Token的KV压缩，忽视了Prompt Token的KV压缩这一实际应用中的主瓶颈；
2. 如$H₂O$算法，虽然能以简单清晰的贪婪策略筛选重要Token，但论文没有过多评估对Prompt Token的适应性，仅提到当输入Token超出2K时，可沿用该算法压缩输入Token；

**洞察：**
1. 将提示按窗口（128-token）划分，并用最后一个提示窗口（观测窗口）计算其对前缀各位置的平均注意力，与真实生成阶段的注意力“重要位置”进行重叠率对比，发现最后窗口的选择与生成时实际用到的特征高度一致，这意味着：**LLM在开始生成前，就已经知道它会关注Prompt Token里的哪些部分**；
2. 将生成部分分为多个窗口，逐段与提示最后窗口所选的重要位置比较，层间重叠率保持较高，表明这种分配模式在生成阶段相当稳定，这意味着：**注意力重要位置在生成过程中保持稳定**，生成过程不会频繁改变要看的Prompt Token位置，因此可以在生成开始前选定并压缩这些位置；

此外，作者还验证：
- 指令内容会影响“重要前缀位置”的选择（问题改变→选择变化），因此压缩策略必须上下文自适应，不能依赖静态权重或固定策略；
- 指令在提示中的位置（前/后）不会显著影响上述可预判性与稳定性；

目标：在不微调的前提下，将提示端KV压缩到近似常量容量，解码复杂度从$O(L_{prompt})$降到$O(const)$，质量尽量不损失；

**设计思路：**

设提示长度：$L_{prompt}$​，划分得到，前缀：$L_{prefix}$​，观测窗口（一般是输入Token的最后一段，用来投票识别输入中的重要Token位置）：$L_{obs}$​，满足：$L_{prompt} = L_{prefix} + L_{obs}$；

投票（Voting）：在观测窗口内，汇聚各个query（观测窗口内每个Token的query）对输入各位置的注意力（对所有头累加），得到对前缀位置的重要性打分；

压缩率参数 $p$：保留的前缀位置数：$k=⌊p×L_{prefix}⌋$

总体流程：
1. 基于观测窗口进行投票，选择重要位置
    - 计算观测窗口内所有query对前缀各位置的$softmax$注意力权重，得到输入位置的重要性矩阵$C$；
    - 对每个头独立地选$Top-k$的前缀位置索引：$I=Topk(C,k)$；
2. 拼接压缩后的 KV
    - 从原始KV中，按索引提取被选中的前缀KV，并与观测窗口的KV全量拼接，作为新的提示端KV缓存，生成阶段仅访问这段常量规模的提示 KV即可；

注意：SnapKV的KV选择策略是逐层逐头的独立选择，对每一层、每一头独立在该头的投票分数上取$Top-k_h$​，不在头间合并统一索引，避免抹平多头功能分工，也就是说：
- 在某一层，某个输入Token的KV可能被某些头保留、在另一些头被驱逐；
- 在另一层，这个Token的KV可能全部被驱逐或部分保留；
- 没有必须在所有层/所有头统一保留或统一驱逐的要求，这种细粒度的稀疏化策略保证了多头的多样性和跨层的差异性；

如图：
![950](imgs/Pasted%20image%2020250901222220.png)

这种按层、按头的稀疏化策略哪怕在解码阶段也是天然支持的，因为多头注意力本来就是按头独立计算注意力，再在头维度拼回去，因此只要为每个头保留下来的是该头自己的KV，解码阶段就用该头自己稀疏化后的历史KV做注意力即可；

补充：聚类（Pooling）
- 仅保留Top-k散点会丢失上下文连续性，影响拷贝/引用能力（如电话号码只留区号，导致后续幻觉）；
- 因而引入一维池化（max/avg pooling，核大小可调，通常5/7/13等）在投票分数上做平滑/扩张，等价于保留高分段及其邻域簇，以保住关键片段的上下文完整性与可复制性；


**总结与拓展**
1. SnapKV提出了一种基于输入Token的KVCache稀疏化策略，基于观测窗口提前识别长文本输入中的关键位置，并开启稀疏化，后续Decode阶段只需要关注稀疏化后的输入KVCache即可；
2. SnapKV按照不同头、不同层独立的进行稀疏化，并通过池化增强片段连续性，兼顾了多头多样性和跨层差异性，稀疏化效果更好；

缺陷：
1. 对输入有额外的稀疏化计算开销，即：在进入解码之前，需要先划分OW等窗口，然后计算OW对于剩余前缀输入的注意力并聚合，用以进行投票选择Top-K，这种动作的成本开销通常远小于随后大幅节省的解码成本，但需要评估短输入场景下的实际收益，找到一个临界点或临界范围；
2. 逐层、逐头的KVCache稀疏化与重排策略，实现复杂度较高，需要精细的工程优化，且对底层算子的侵入性可能较大；
3. 尽管论文说明OW窗口保留在输入末尾即可针对不同类型的输入文档具备不错的效果，但仍然无法完全保证不同的输入场景下，当前的SnapKV算法都能保证较高的准确性和效率；

### PyramidKV

论文地址：[PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling](https://arxiv.org/html/2406.02069v4)
代码仓：[https://github.com/Zefan-Cai/PyramidKV](https://github.com/Zefan-Cai/PyramidKV)

PyramidKV提出在上下文场景下，现有的KVCache稀疏化策略存在以下缺陷：
1. 传统KV缓存稀疏方法（如固定大小的KV缓存或基于注意力分数的稀疏化）通常对所有层采用统一的压缩策略，忽略了不同层的注意力模式差异；
2. 这些方法在压缩KV缓存时，可能丢失关键信息，导致生成质量下降；
 
**洞察：**
基于以下现象：**长上下文任务下，不同层的attention score的分布呈现金字塔型的信息漏斗模式；**
- 底层（Broad Spectrum）：在模型的初始层（如Layer 0），注意力权重分布非常均匀，模型会关注几乎所有的输入令牌，进行广泛的全局信息收集；
- 中层（Localization）：随着层级加深（如Layer 6-18），注意力开始变得更加**局部化**，模型会将注意力集中在特定的文档或段落内部，进行更精细的信息聚合；
- 高层（Concentration）：在模型的顶层（如Layer 24-30），注意力呈现出**极度稀疏**的**Massive Attention**现象，绝大部分注意力都集中在极少数几个关键令牌上，这些令牌已成为高度浓缩的信息载体，为最终的答案生成提供核心依据；

PyramidKV与现有KV稀疏化方法的比较：
![950](imgs/Pasted%20image%2020250901222813.png)
1. 不启用稀疏化，保留所有KVCache；
2. 启用StreamingLLM稀疏化算法，保留输入的前N个Token，以及滑窗内Token；
3. 启用H2O或SnapKV，基于累计注意力分数等策略选择重要位置的Token；
4. PyramidKV：将更多的缓存预算分配给较低层，将更少的缓存预算分配给较高层，这种KV缓存选择方法更好地符合在多层Transformer中观察到的注意力稀疏性增加；

**设计思路：**

基于金字塔信息漏斗的发现，PyramidKV提出了动态KV缓存压缩策略：
1. 非均匀缓存分配：摒弃传统的统一缓存大小分配，采用层级差异化策略
2. 底层优先原则：为信息分散的低层分配更多缓存空间
3. 顶层精简策略：为信息集中的高层分配较少缓存空间
4. 几何形状匹配：缓存分配呈金字塔形状，与注意力模式相匹配

具体实施：
1. 设总缓存预算为: K，以等差数列的递减方式逐层分配不同层的缓存预算；
2. 每层选择Token时，先选择Last-N个近期Token，其余通过Attention Score选择top-k，然后丢弃剩余Token；

**总结与拓展**

1. 以一种不同的视角(信息呈金字塔方式向顶层集中)提出层级的差异化KVCache稀疏化策略；
2. 层内稀疏时，策略类似H2O，兼顾长短距离依赖，整体工程化复杂度较低；

### Quest
论文地址：[Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](https://arxiv.org/html/2406.10774)
代码仓：[https://github.com/mit-han-lab/Quest](https://github.com/mit-han-lab/Quest)

**背景：**
1. 既有研究表明，少部分关键Token在注意力中贡献主导，稀疏化注意力是可行的，但多数方法是与查询无关（query-agnostic）的静态稀疏，或基于启发式选取，难以在长上下文任务中稳定保持准确度；
2. 关键性的Token是与当前query强相关的动态属性，同一段上下文对于不同query的重要性差异很大，如果忽略query依赖，容易选错关键Token，造成准确度损失；

**洞察：**
1. 关键性是**查询依赖**的(**query-aware**)：不同query对于历史Tokens的关注分布差异显著，因此必须在推理过程中时刻感知当前query，而不是用固定规则选择关键Token；

![](imgs/Pasted%20image%2020250901222900.png)
如图，基于Query-Aware的KVCache稀疏化策略及其局限性：
1. 不启用稀疏化，复杂度更高，但准确率也高；
2. 基于H2O等静态稀疏化策略，复杂度低，但针对不同查询，关键Token会动态变化，但由于Token无法被召回，导致准确率骤降；
3. 维持全量KVCache，保证可召回，针对不同查询筛选不同的关键Token，即保证了更低的复杂度，也兼顾准确性；

**设计思路：**

1. 先把整段上下文的KV缓存按照固定页大小 ($P$) 进行分页组织（如每页 256/512 个 token，按层与头分别切分并顺序存放），在每页写满或冻结时立即计算该页Key矩阵 ($K \in \mathbb{R}^{P \times d_k}$) 的逐维最小值与最大值向量 ($K^{\min}, K^{\max} \in \mathbb{R}^{d_k}$) 作为元数据摘要，并用低精度（如 fp8/int8）存储，以便后续只用这些常数级元数据就能对整页的重要性进行近似评估，从而把注意力选择的粒度从 token 降到 page，减少随机访问与筛选成本，同时保持对可能高相关区域的“上界”覆盖；
    
2. 在推理时对每个新到的 Query 向量 ($q \in \mathbb{R}^{d_k}$) 进行页面级打分：利用区间内积上界思想，对每一页计算 ($S_{\text{ub}}(q,\text{page})=\tfrac{1}{\sqrt{d_k}}\sum_{j=1}^{d_k}\max(q_j K^{\max}_j, q_j K^{\min}_j)$)，它给出了该页内可能出现的最大注意力对数值的紧致上界（不需要读取整页 $K$）；此分数天然是“查询感知”的，因为其与 $q$ 分量逐维相乘并考虑符号，能对不同查询动态改变页的重要性排序，同时计算代价为 ($O(d_k)$) 且只依赖紧凑元数据，降低筛选成本；
    
3. 基于上述分数对全体页面做 Top-$K$ 选择，得到被认为最可能与当前 $q$ 高度相关的页面集合 ($\mathcal{S}$)，可按“每头独立 Top-$K$”或“多头融合后统一 Top-$K$”两种策略执行，并加入小规模的安全兜底（例如强制保留最近的若干页以覆盖短程依赖或热点段落），使选择既兼顾长程信息检索又避免遗漏关键的局部上下文；这一阶段仅涉及对元数据数组进行线性扫描与选择（可用 partial sort/heap 实现），不会触发大规模 KV 实体搬运；
    
4. 接着只对入选的页面 ($\mathcal{S}$) 执行真实注意力计算：从内存（或 HBM）中批量、按页聚合地读取其对应的 ($K_{\mathcal{S}}, V_{\mathcal{S}}$) 张量，按常规缩放点积注意力公式 ($\text{softmax}\big((q K_{\mathcal{S}}^\top)/\sqrt{d_k}\big) V_{\mathcal{S}}$) 得到输出，这一步的计算与带宽消耗相较全量注意力显著降低到与 ($|\mathcal{S}|\cdot P$) 成正比；结合页连续布局与预取可以把随机 I/O 转为顺序/批量访问，配合流式计算管线在 GPU 上实现高吞吐的稀疏注意力，从而在长上下文下获得明显的延迟与带宽优势；
    
5. 最后对运行过程进行轻量自适应与一致性维护：在不改变模型权重的前提下，动态调整 $K$（页面数量）、页大小 $P$ 与兜底页数等运行超参，使其在不同任务/长度/硬件带宽下实现准确率与吞吐的最优折中；同时在生成阶段增量维护新页的元数据（随写随建）、保留上一时刻的选择热度作为先验偏置、并在极端场景（如分数分布过于平坦或 Top-$K$ 过小）时回退到更保守的设置（增大 $K$ 或加入全局锚页），以确保在保持显著加速的同时，尽量逼近全量注意力的质量上限；

![900](imgs/Pasted%20image%2020250901222959.png)

具体执行逻辑如上图，一个可视化演示如下：

设定：
- 序列长度：8个token
- Key向量维度：4维
- 页大小：2个token（实际中通常256+）
- 需要选择Top-2页面

Stage 1：原始KV Cache分页与元数据构建

原始Key矩阵 (8×4):

```
Token0: [2.0, -1.0,  3.0,  0.5]
Token1: [1.5,  2.0, -0.5,  1.0] 
Token2: [0.8,  1.2,  2.5, -0.8]
Token3: [2.2, -0.5,  1.8,  0.3]
Token4: [-1.0, 3.5,  0.2,  2.1]
Token5: [1.8, -2.0,  1.5,  0.9]
Token6: [0.3,  0.8, -1.2,  3.2] 
Token7: [2.5,  1.1,  0.9, -0.4]
```

按页大小=2分页后：

```
Page 0 (Token 0-1): 
[[2.0, -1.0,  3.0,  0.5]
 [1.5,  2.0, -0.5,  1.0]]

Page 1 (Token 2-3):
[[0.8,  1.2,  2.5, -0.8] 
 [2.2, -0.5,  1.8,  0.3]]

Page 2 (Token 4-5):
[[-1.0, 3.5,  0.2,  2.1]
 [1.8, -2.0,  1.5,  0.9]]

Page 3 (Token 6-7):
[[0.3,  0.8, -1.2,  3.2]
 [2.5,  1.1,  0.9, -0.4]]
```

计算每页的元数据（逐维min/max）：

```
Page 0 元数据:
K_min = [min(2.0,1.5), min(-1.0,2.0), min(3.0,-0.5), min(0.5,1.0)] = [1.5, -1.0, -0.5, 0.5]
K_max = [max(2.0,1.5), max(-1.0,2.0), max(3.0,-0.5), max(0.5,1.0)] = [2.0,  2.0,  3.0, 1.0]

Page 1 元数据:
K_min = [0.8, -0.5,  1.8, -0.8]
K_max = [2.2,  1.2,  2.5,  0.3]

Page 2 元数据: 
K_min = [-1.0, -2.0,  0.2,  0.9]
K_max = [1.8,   3.5,  1.5,  2.1]

Page 3 元数据:
K_min = [0.3,  0.8, -1.2, -0.4] 
K_max = [2.5,  1.1,  0.9,  3.2]
```

Stage 2：Query感知打分

假设当前Query向量：$q = [1.0, -0.5, 2.0, 1.5]$

对每页计算上界分数： 对每一维j，计算 $max(q[j] \times K_{min}[j], q[j] \times K_{max}[j])$

**Page 0打分**：

```
维度0: max(1.0×1.5, 1.0×2.0) = max(1.5, 2.0) = 2.0
维度1: max(-0.5×(-1.0), -0.5×2.0) = max(0.5, -1.0) = 0.5  
维度2: max(2.0×(-0.5), 2.0×3.0) = max(-1.0, 6.0) = 6.0
维度3: max(1.5×0.5, 1.5×1.0) = max(0.75, 1.5) = 1.5
Page 0 分数 = (2.0 + 0.5 + 6.0 + 1.5) / √4 = 10.0 / 2 = 5.0
```

**Page 1打分**：

```
维度0: max(1.0×0.8, 1.0×2.2) = 2.2
维度1: max(-0.5×(-0.5), -0.5×1.2) = max(0.25, -0.6) = 0.25
维度2: max(2.0×1.8, 2.0×2.5) = 5.0  
维度3: max(1.5×(-0.8), 1.5×0.3) = max(-1.2, 0.45) = 0.45
Page 1 分数 = (2.2 + 0.25 + 5.0 + 0.45) / 2 = 3.95
```

**Page 2打分**：

```
维度0: max(1.0×(-1.0), 1.0×1.8) = 1.8
维度1: max(-0.5×(-2.0), -0.5×3.5) = max(1.0, -1.75) = 1.0
维度2: max(2.0×0.2, 2.0×1.5) = 3.0
维度3: max(1.5×0.9, 1.5×2.1) = 3.15  
Page 2 分数 = (1.8 + 1.0 + 3.0 + 3.15) / 2 = 4.48
```

**Page 3打分**：

```
维度0: max(1.0×0.3, 1.0×2.5) = 2.5
维度1: max(-0.5×0.8, -0.5×1.1) = max(-0.4, -0.55) = -0.4
维度2: max(2.0×(-1.2), 2.0×0.9) = max(-2.4, 1.8) = 1.8
维度3: max(1.5×(-0.4), 1.5×3.2) = max(-0.6, 4.8) = 4.8
Page 3 分数 = (2.5 + (-0.4) + 1.8 + 4.8) / 2 = 4.35
```

Stage 3：Top-K选择
各页分数排序：
- Page 0: 5.0 (最高)
- Page 2: 4.48 (第二)
- Page 3: 4.35
- Page 1: 3.95 (最低)

选择Top-2页面：Page 0 和 Page 2

Stage 4：稀疏注意力计算
只读取被选中页面的KV数据：

```
选中的K矩阵 (4×4，来自Page 0和Page 2):
[[2.0, -1.0,  3.0,  0.5]   # Token 0
 [1.5,  2.0, -0.5,  1.0]   # Token 1  
 [-1.0, 3.5,  0.2,  2.1]   # Token 4
 [1.8, -2.0,  1.5,  0.9]]  # Token 5
```

执行注意力计算：$softmax(q·K^T / √4) · V$

核心洞察：元数据的作用

为什么min/max有用？

对于Query $q = [1.0, -0.5, 2.0, 1.5]$，我们看Page 0第2维：

- Page 0在第2维的范围是$[-0.5, 3.0]$
- $q[2] = 2.0 > 0$，所以与该维最大值3.0相乘得到最大可能贡献：2.0 × 3.0 = 6.0
- 这个6.0是该页在第2维上能产生的注意力贡献上界

为什么这样筛选有效？

Page 1被淘汰是因为其分数较低，但我们来验证：如果真正计算Page 1内两个token与query的内积：

```
Token 2: [0.8, 1.2, 2.5, -0.8] · [1.0, -0.5, 2.0, 1.5] = 0.8 - 0.6 + 5.0 - 1.2 = 4.0
Token 3: [2.2, -0.5, 1.8, 0.3] · [1.0, -0.5, 2.0, 1.5] = 2.2 + 0.25 + 3.6 + 0.45 = 6.5
```

而Page 0的实际内积：

```
Token 0: [2.0, -1.0, 3.0, 0.5] · [1.0, -0.5, 2.0, 1.5] = 2.0 + 0.5 + 6.0 + 0.75 = 9.25
Token 1: [1.5, 2.0, -0.5, 1.0] · [1.0, -0.5, 2.0, 1.5] = 1.5 - 1.0 - 1.0 + 1.5 = 1.0
```

Page 0确实包含了最高的注意力值(9.25)，证明我们的元数据打分策略的正确性，这个例子展示了Quest的核心价值：**通过轻量的元数据预筛选，避免读取和计算不重要的KV数据，将注意力计算从全量8×8降低到稀疏4×8，既保持了准确性又显著降低了计算和内存成本。**

注意！这里可以会有误解：既然只需要基于max算各个维度的注意力分数上界，为什么Quest还需要维护k_min的元数据呢？？只维护k_max不就行了？

核心原因：上界要对所有查询向量$q$都成立，而$q$的分量可以为正也可以为负，逐维上界不是简单的$q^j⋅K_j^{max}⁡$​，而是:
$$
\max\big(q_j \cdot K^{\max}_j, q_j \cdot K^{\min}_j\big),
$$
因为不同符号下，能达到更大内积贡献的端点不同。

比如，设某一页在某维的取值范围是$([K^{\min}_j, K^{\max}_j] = [-5, 1])$，而查询该维为$(q_j = -3)$；
- 真实可达到的该维最大贡献是$max\{−3⋅(−5),−3⋅1\}=max\{15,−3\}=15$；
- 如果你只用$(K^{\max}_j = 1)$，会得到$(-3 \cdot 1 = -3)$，把上界错压到了-3，与真实可能达到的15相差巨大；
- 结果：这页在总分中的贡献被严重低估，极可能被Top-K筛掉，漏掉真正重要的内容；

**总结与拓展**
1. Quest提出了基于Query感知的KVCache稀疏化策略，相较于H2O、StreamingLLM、SnapKV等静态稀疏方法，Quest能够基于不同的Query感知不同的关键位置Token，从而将其动态召回，在兼顾KVCache稀疏的基础上，做到了更高的准确度；
2. Quest的Top-K选择策略基于Page为粒度进行注意力打分，并维护了每个Page的各个维度的注意力分数上界和下界，基于不同的查询向量，能够在较短时间开销内命中Top-K的Page，并且由于基于Page划分和管理KVCache，在逻辑上更连续，更有利于吞吐；

缺陷：
1. Top-K选择策略可能存在误差场景，可能较宽松，极端情况下会高估某些页的潜力，造成Top-K名额占用，从而影响模型精度，需要按经验性设定K以平衡速度/精度；
2. 未考虑跨层或多头的差异化KVCache稀疏策略，并且由于基于动态稀疏，支持KVCache召回，并没有实际上减少KVCache的显存占用，主要的收益在于减少了HBM读带宽压力；

实际应用中建议结合跨层差异化稀疏策略，如更高层稀疏程度更激进；

### Ada-KV
论文地址：[Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference](https://arxiv.org/html/2407.11550)
代码仓：[https://github.com/FFY0/AdaKV](https://github.com/FFY0/AdaKV)

**背景：**
1. 长上下文推理的瓶颈：自回归LLM在长序列推理中需要为每一层多头注意力维护不断增长的KV Cache（Key/Value），导致显存占用与I/O访问延迟迅速上升，例如8B模型在2M上下文下的KV可能高达数百GB级；
2. 现有主流路径：通过“运行时剔除不重要的缓存元素”（KV eviction）降低缓存规模，在Top-k类方法中，基于观察到的注意力权重挑选“关键”的历史token的KV保留，其余清理；
3. 核心缺口：几乎所有方法默认“对每个注意力头均匀分配预算”（每头保留相同数量的KV条目），但注意力头存在显著异质性：有的头极度稀疏（少量token集中了大部分注意力），有的头分散（需要相当比例的token才覆盖主要注意力），均匀分配会在稀疏头上浪费预算、在分散头上预算不足；


**洞察：**
1. 从“给定预算下的最优选择”到“预算也可优化”：既然Top-k在固定每头预算时最优，那么进一步的改进空间在于“如何按头分配预算”；
2. 论文提出“头级自适应预算分配”（Ada-KV），通过对同一层所有头的注意力权重进行跨头排名，在全层总预算$B$的约束下，把$B$个最大权重按归属头的频次分配成各头预算$B \times i$，这样自然把更多预算分给注意力分散的头，把较少预算分给稀疏头；

**设计思路：**

区分不同的注意力头：
- 稀疏头（Sparse Heads）：注意力高度集中于少数几个token，保留少量缓存即可维持绝大部分注意力权重；
- 分散头（Dispersed Heads）：注意力分散在大量token上，需要保留更多缓存才能避免信息损失；

![800](imgs/Pasted%20image%2020250901223219.png)

按Attention Head分配KV cache预算，head内部进行token evicition。

固定预算K，将每层所有attention head的score铺平，然后按照每个头入选该层Top-K的权重数量来分配预算，每个head在预算内进行token evicition，可以与SnapKV/PyramidKV结合。

本质上，就是对已有Top-K算法的增强，即引入了额外的按头计算配额的设计；

**总结与拓展**
1. Ada-KV的设计思想较为简单，即：不同的注意力头具有不同的注意力集中模式，因此应该根据每个头的特性来自适应地分配缓存预算，而不是简单的均匀分配；

### GemFilter
论文地址：[Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/html/2409.17422)
代码仓：[https://github.com/SalesforceAIResearch/GemFilter](https://github.com/SalesforceAIResearch/GemFilter)

**洞察：**
1. 模型在生成答案前的早期层已经能凭注意力矩阵定位关键的输入Token，具体而言：
    - 以查询最后一个Token的注意力分布（attention matrix的最后一行）为线索，早期层就能“聚焦”到与问题相关的上下文位置；
    - 能显著暴露关键信息的早期层称为“过滤层”（filter layer），比如在LLaMA 3.1 8B上，第13–19层均可作为有效过滤层；
2. 这些早期层不仅指向关键信息，甚至在中早期层已显式总结了所需信息——这说明早期层具备有效的信息蒸馏能力；
3. 这意味着，我们无需等待模型运行完所有层，就可以在早期阶段“预知”哪些token是解决问题所必需的，这些早期层（通常是模型深度1/3到1/2处）的注意力矩阵，特别是最后一个查询（query）token的注意力分数，可以作为一个高效的“过滤器”，用于从海量输入中筛选出精华部分；

### DuoAttention
论文地址：[DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://arxiv.org/html/2410.10819)
代码仓：[https://github.com/mit-han-lab/duo-attention](https://github.com/mit-han-lab/duo-attention)

洞察：
1. 注意力头存在功能分工的稳定模式，可划分为两类：
    - 检索头（Retrieval Heads）：负责跨段检索上下文中语义相关 token，对长上下文能力至关重要，必须保留对全序列的“全注意力+完整 KV”；
    - 流式头（Streaming Heads）：主要关注attention sinks + 近期 token，对中间大量历史token几乎不关注，因此其KV可安全压缩为常数长度（仅保留头尾与近期）；
2. 关键观察：如果仅对流式头进行“中间 token”裁剪，模型的长程检索能力几乎不受影响；反之如果裁剪检索头，性能显著下降；

设计思路：
1. 涉及训练阶段工作，暂略；

还有许多其它类似的KV稀疏化论文诸如：

### LeanKV
K、V矩阵采用不同的稀疏化策略

### ClusterKV
完整KV cache offload到CPU中，将KV cache进行K-means聚类，计算Query与聚类中心的相似度，按照相似度进行召回；

### ShadowKV

### Cake

### FreeKV

### RaaS
论文地址：[RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning](https://arxiv.org/html/2502.11147)
代码仓：[https://github.com/DerekHJH/raas](https://github.com/DerekHJH/epic)

华为云联合北大实验室近期发布的一篇论文

**背景**

- 长序列推理的瓶颈
    - 大型语言模型在数学、编程、链式思考等推理任务中产生很长的输出（长解码），导致每步计算注意力需与全部已生成词元交互，单步复杂度约为 $O(N)$，整段生成为 $O(N^2)$
    - 虽然KV缓存避免了重复计算，但缓存与计算仍随历史长度$N$线性增长：时间 $O(N)$，内存 $O(N)$
- 既有稀疏化方法的三难困境（准确率-时间-内存）
    - Dense：准确高，但时间/内存均为 $O(N)$
    - StreamingLLM/Sink：时间/内存降至 $O(L)$，但激进裁剪导致准确率明显下降
    - H2O：宣称 $O(L)$ 时间/内存，但工程上难以与高效内核兼容，且容易保留“过时信息”影响精度
    - Quest：时间 $O(L)$ 且精度高，但为动态保护关键Token而保留全量KV，内存仍为 $O(N)$
- 问题聚焦
    - 论文专注优化解码阶段（Decode），因为推理场景中解码时间占比可达95%甚至更多，且近期推理型方法（如 o1/o3、DeepSeek R1）显著拉长了解码长度

常见稀疏算法对比($N$表示序列长度， $L$表示缓存预算，其中$L≪N$)：
![](imgs/Pasted%20image%2020250908005417.png)


**洞察**

1. 推理任务的注意力新模式
	- 在对 Qwen2.5-Math-7B-Instruct 的多层多头注意力进行人工审视后，作者总结两类关键行为：
        - **里程碑（Milestone）词元**：
            - 出现后在一段时间内被频繁关注，随后注意力逐步衰退，并不再回升；
            - 类比推理中的“引理/中间结论”，被后续步骤消费后自然淡出；
            - 注意力图上表现为“逐渐变暗的一列”；
        - **凤凰（Phoenix）词元（类似凤凰涅槃重生）**：
            - 曾长期低注意力甚至被驱逐，但后来“死而复燃”被再次强关注；
            - 多来自前缀（尤其用户问题），在结论处被回溯引用；
2. 经验总结
    - 不当丢弃解码区的“里程碑”会让模型丢失思路、反复重推，精度和长度都会恶化。
    - 凤凰词元主要来自短前缀，因此保留前缀 KV 可在不增加太多预算的同时保护关键语义。
    - 预算充足（如 $L\ge 512$）时，解码区几乎不再出现凤凰；预算很小（如 $L=64$）时，凤凰仍可能出现。

注意力模式洞察：
![](imgs/Pasted%20image%2020250908005701.png)

**设计**

Select策略：
1. Decode阶段里程碑Token保活（LRU 近似）
	- 用“最近被使用”的信号近似里程碑活跃度：每步将注意力分数高于“中位数”的页面标记为被使用并更新时间戳；缓存满时按 LRU 淘汰最旧者；
	- 真正的里程碑会在生命期内持续刷新时间戳，变“过时”后才自然被淘汰；
2. 前缀区全保留
	- 不淘汰前缀（提示）KV，避免丢失凤凰词元；
	- 推理任务里前缀较短，此举以有限成本保护关键信息；

执行流程说明：

1.术语说明
- 序列与分区
    - 前缀（prefill）长度：$N_p$
    - 解码（decode）步数：$T$（动态增长）
    - 总长度：$N=N_p+T$
- 页级缓存（与 vLLM 一致）
    - 页大小：$\text{page\_size} = P$（典型 $P=16$）
    - 第 $j$ 页的键/值张量：$K_j \in \mathbb{R}^{P \times d_k}$，$V_j \in \mathbb{R}^{P \times d_v}$
    - 缓存预算（页数上限）：$L$
- 注意力与打分（简化为单头）
    - 当前步查询：$q_t \in \mathbb{R}^{d_k}$
    - 标准缩放点积注意力对单 token 的打分：$\alpha_i = \frac{q_t^\top k_i}{\sqrt{d_k}}$

2.页级代表分数：从token到页的可比尺度

为了兼容高效注意力内核与页式管理，RaaS对“页”而非每个token进行存活管理，为此需将页内token的注意力打分聚合为“代表分数” $s_j$，常见两种聚合：

- 最大池化（偏“召回关键点”） $$ s_j^{\max} = \max_{i \in \text{page } j} \left( \frac{q_t^\top k_i}{\sqrt{d_k}} \right) $$
    
- 均值池化（偏“稳健抗噪”） $$ s_j^{\text{mean}} = \frac{1}{P}\sum_{i \in \text{page } j} \left( \frac{q_t^\top k_i}{\sqrt{d_k}} \right) $$
工程上与Quest一致地可用“代表 key”近似（缓存页内代表 Key），本分析默认使用 $s_j = s_j^{\max}$，也可采用加权混合以折中： $$ s_j = \alpha \cdot s_j^{\max} + (1-\alpha)\cdot s_j^{\text{mean}},\quad \alpha \in [0,1] $$

3.中位数阈值与“最近使用”判定（识别活跃里程碑）

在步$t$对所有在库页计算代表分数集合 $S_t={s_1,\dots,s_M}$（$M$ 为当前缓存中的页数），以中位数作为阈值： $$ \theta_t=\mathrm{median}(S_t) $$
将分数不低于阈值的页视为“被使用”（近似活跃里程碑）： $$ \text{used}(j,t)=\mathbb{I}[,s_j \ge \theta_t,] $$

以“被使用”为触发信号刷新页的 LRU 时间戳（最近使用时间）： $$ \text{ts}(j) \leftarrow \begin{cases} t, & \text{若 } \text{used}(j,t)=1 \ \text{ts}(j), & \text{否则} \end{cases} $$

直觉：真正的“里程碑”页在“生命期”内会频繁落入上半分位（$s_j \ge \theta_t$），从而连续刷新 $\text{ts}(j)$；当其价值被后续步骤消费完后，$s_j$ 会下降并停止刷新，继而在竞争中自然淘汰。

4.前缀全保与可驱逐集合（规避凤凰风险）

- 将所有Prefill页标记为“不可驱逐”： $$ j \in \mathcal{P} \Rightarrow \text{evictable}(j)=0 $$
    
- 仅Decode区页可被驱逐： $$ j \in \mathcal{D} \Rightarrow \text{evictable}(j)=1 $$
动机：凤凰页主要来自前缀（例如用户问题陈述），在最终总结时可能被回溯强引用，前缀全保以小成本（前缀相对短）避免“晚期回看失败”的重大精度损失。


5.缓存满载时的 LRU 驱逐策略（只淘汰解码页）

当写入新解码页将超出预算 $L$ 时，从可驱逐集合（解码页）中选择时间戳最旧者淘汰： $$ j^\star = \arg\min_{j \in \mathcal{D},, \text{evictable}(j)=1} \text{ts}(j) $$

随后在该位置写入新页，并将其时间戳初始化为 $t$，这相当于保持“最近使用”的解码页存活，符合“里程碑保活”的目标。

6.稀疏读写与复杂度

- 单步注意力仅与至多 $L$ 个页交互：时间复杂度 $O(L)$
- 新页写入引发的驱逐与分配摊还为 $O(1)$
- 解码阶段总复杂度（$T$ 步）：$O(TL)$
- 峰值内存受页上限约束：$O(L)$

对比 Dense（单步 $O(N)$、总 $O(N^2)$、内存 $O(N)$），RaaS 将解码阶段的计算与内存成本均绑定在常数$L$上。

**总结**

设计思想类似Quest，但不持久化KV，不召回，基于近似LRU策略进行驱逐。

可能的缺陷：
1. 不支持持久化和动态召回，可能不太好兼容多轮对话等场景，但这里可以在工程设计上优化；
2. 对短前缀-长解码的推理任务最优；长前缀需要与Quest组合以避免预算被前缀占满；

实际上，可以结合RaaS与Quest，将RaaS作为Decode-Only策略；Prefill采用Quest，形成混合流水线。

### SamKV
论文地址：[Sparse Attention across Multiple-context KV Cache](https://arxiv.org/html/2508.11661v1)

**背景：**

- 长上下文推理的代价高昂：LLM 在处理长序列时，推理时延与显存占用显著上升；
- 单上下文方法（如 SnapKV、InfLLM）假设按因果顺序在同一上下文内增量生成，适配性有限；
- RAG的多上下文现实：检索得到的多个文档分别预填充并各自生成 KV Cache，跨文档之间没有生成期的交叉注意力，已有多上下文方法（CacheBlend、EPIC）通过“选择性重算”弥补跨注意力缺失，但必须携带“完整 KV”，显存无法下降；

痛点归纳：
- 单上下文稀疏化：能省算与省显存，但无法解决多上下文“跨文档注意力缺失”；
- 多上下文选择性重算：能补注意力缺失，但显存不降（携带全量 KV）；

**洞察：**

- 多上下文存在“跨文档共识”（inter-document consensus）：检索来的上下文往往彼此重叠、对同一问题的关键信息在多篇文本中重复出现，稀疏选择时如果只依赖用户query的表示，难以精准识别这些“跨文档共享”的关键信息；
- 注意力“锚点”与“注意力汇聚（sink）”规律可用：初始位置与局部位置的 KV 块对推理影响巨大；中段存在注意力集中或“显著 token”现象，可用于判别重要块；
- 稀疏化与选择性重算是可组合的：若先跨文档感知稀疏，再对极少量关键token局部重算，可同时解决显存与跨注意力缺失的矛盾；

**设计思想**

SamKV是面向“多上下文 KV Cache”的首个系统化稀疏化方案，由三大模块构成：

1. 个性化查询向量（Personalized Query Embedding）
- 先用“初始块 + 局部块”的压缩 KV 进行增量预填，得到通用查询向量$Qque$；
- 再把其他上下文的“局部 Q Cache”作为微弱偏置注入，构建针对当前文档的“个性化”查询向量 Q^doc-i​，以体现跨文档共识；

2. 重要 KV 选择（KV Selection）

- 固定保留初始与局部块（不稀疏），对中段块进行稀疏化。
- 以“锚点块”为参照，计算动态 Top-P 选择比例：只保留“比锚点更重要”的块；并跨文档进行归一化与对比，进行全局筛选。

3. 局部重算（Recomputation）

- 为弥补独立预填导致的跨文档注意力缺失，只对稀疏子集中的关键 token 局部重算（含初始/局部/高注意力 token）。
- 解决“跨层选块不对齐”的工程问题：用填充空块对齐，再按两条重算规则自底向上最小化计算。
- 支持“覆盖”或“融合”两种 KV 更新策略；融合以余弦相似度自适应配比，兼顾新旧信息。

目标：在保证精度的前提下，将需携带的多上下文 KV 降到原始的约 15%，显著提升吞吐与降低显存。


## 3.社区现状及发展
基于大部分的论文研究成果，可以分析得出，当前的KV稀疏方法在工程设计上，主要涉及到两个方面：
1. Selector
2. Recaller

Selector负责Token的选择或驱逐，它又能细分为静态或动态两个方面，其中：
1. 静态稀疏化策略认为在不同的时刻有一部分内容一直不太重要，因此直接将其驱逐，比如StreamingLLM、SnapKV、H2O等算法都归属于静态稀疏；
2. 动态稀疏策略主要认为不同的时刻重要的内容是会随之变化的，因此该类策略需要保存全部的KVCache，但在运算时仅仅挑选当前时刻重要的内容参与运算，比如Quest、ClusterKV等都可以归属于动态稀疏；

可以发现，动态稀疏方法需要一个支持KVCache可召回的必要组件，即：Recaller，它在系统设计上是可选的，一般仅在动态稀疏策略下支持。

同时，在超长上下文场景下，如果需要保留全量KVCache并支持可召回，一般还需要考虑KVCache的Offload策略，同时，因为涉及到CPU <--> GPU的数据传输，在工程设计上会更复杂，比如需要搭配异步调度策略去重叠召回开销等。

理论上，静态稀疏和动态稀疏是正交的，一般可以混合使用，比如可以先用静态稀疏性将KVCache压缩到50%，再用动态稀疏性每次仅选取20%的KVCache参与运算。

**LMCache**

LMCache的主线版本当前仅支持以CacheGen的方式实现流式的KVCache压缩策略，但不支持KVCache的稀疏化。

其非主线开发分支，对H2O还有AttentionSink等稀疏化算法有相应支持：

使用方式：

```shell
git clone -b compaction git@github.com:LMCache/LMCache.git
git clone -b compact git@github.com:LMCache/lmcache-vllm.git
pip install -e .

LMC_COMPACTOR=True COMPACTOR_TYPE={Sink, H2O} VLLM_ATTENTION_BACKEND=XFORMERS python long_gen.py
```

**KVPress**

Nvidia开源了一个关于KVCache稀疏化的算法实现库：[kvpress](https://github.com/NVIDIA/kvpress/tree/main)，当前版本为v0.2.10.

kvpress实现了当前多数经典且行之有效的KVCache稀疏化方法，并提供了标准的SCBenchmark测试脚本用以评估稀疏化程度对性能以及质量的影响。

基于kvpress对稀疏化方法进行性能评估，结果如下图：

![850](imgs/Pasted%20image%2020250902011515.png)
![800](imgs/Pasted%20image%2020250902012206.png)

如图，可以看出，基于AdaKV的SnapKV稀疏化方法或者ExpectedAttention稀疏化方法、ObservationAttention等，在稀疏化压缩率为50%时，仍能保持接近80%-90%的生成质量。

## 4.实施方向与设计

KVCache稀疏化的适配场景：**超长上下文**。

这里需要先分析清楚一个界限或者边界，即：
1. 不同的稀疏化方法各自能发挥作用的上下文长度边界是多少？比如2K、4K、16K等等；
2. Top-K等配额以及相关超参数的最佳配置；
3. 不同的稀疏化方法的模型适应性？比如会不会存在一些算法在特定的一系列模型上效果不佳；

然后，建议首先引入的稀疏化方法包括：
- 静态稀疏：滑窗、H2O、SnapKV、PyramidKV、Ada-KV
- 动态稀疏：Quest

在稀疏算法的引入上，建议先支持静态稀疏，再考虑支持动态稀疏，最后才是混合稀疏策略。

设计上，要考虑KVCache稀疏性与推理引擎已有的加速优化技术的有机统一与结合，如：
1. 与KVCache量化的结合（两者本身都属于KVCache的压缩技术，本质上互不冲突，可以结合使用）
2. 与PrefixCaching技术的结合：如果是静态稀疏方法，KV被永久驱逐，可能会造成与前缀缓存结合的相性不佳，不过可以将KV稀疏化只应用于Decode阶段从而解决该问题，或者考虑更优的解决方案；如果是动态稀疏方法，因为KVCache被全量保留，对前缀缓存的适配性更好；
3. 结合PD分离：PD分离目前一个比较重要的问题在于KV Cache的传输，如果结合KVCache稀疏化，是不是能够提升传输速度，弥补因为传输瓶颈导致的性能损失问题呢？这里可能还需要再好好想想怎么结合。

还需要关注一个问题，探索Query无关的KVCache稀疏化策略，这可能会在适配更多应用场景时变得重要。

因为带着问题进行稀疏化，一般会有更好的稀疏化效果，然而这在特定的应用场景会很受限，比如：
- 共享前缀缓存场景，稀疏化该共享前缀，此时是不知道用户Query的；
- Chunked Prefill场景稀疏化前面几个Chunk，此时也无法知道用户Query（不过这里可以从后续Chunk中取，因此问题可能不大）；
- 多轮场景在稀疏化第一轮的KVCache时，也是无法知道用户后续轮次的Query的，就有可能导致后续轮次的质量效果变差；

总之，以上这些依赖Query-Aware的应用场景下，可能不针对特定问题的稀疏化策略就会显得更加重要，比如Quest等。

最后，给出稀疏KVCache引入vLLM等生产级推理引擎的设计分析。

对于静态稀疏，由于KV被永久驱逐，并且甚至不同层、不同注意力头的驱逐位置均不相同，这也就导致了在vLLM的Block划分机制下，如果直接应用稀疏化，Block内部可能存在大量的不均匀KV占用，产生大量的内存碎片，反而拉低了推理性能。

**1.稀疏KVCache的显存管理

对于静态稀疏，由于KV被永久驱逐，并且甚至不同层、不同注意力头的驱逐位置均不相同，这也就导致了在vLLM的Block划分机制下，如果直接应用稀疏化，Block内部可能存在大量的不均匀KV占用，进而产生大量的内存碎片，反而拉低了推理性能，与FA等注意力优化内核也不兼容。

解决该问题，当前初步设想有以下几种解决思路：
- 可以考虑在vllm的KVCacheManager等显存分配组件中引入重分配的能力，即：对于稀疏后的KVCache，通过重分配将逻辑空间再次连续，以减少内存压缩后的内存碎片，这种方式与FA等内核也更适配，因为FA的访存模式更偏好连续、规则的访问模式；
- 直接设定BlockSize=1，最简单的方案，但可能开销会更大，因为等同于丧失了PageAttention的优势；
- 重构vllm的显存管理机制，以更好的兼容稀疏KVCache，比如可以参考ellm的思想引入弹性显存管理框架，将Tensor作为最底层的显存分配与管理单位，但是该方案实现最复杂；

对于动态稀疏，由于KV仍被完整保留，但在长上下文场景下，这个显存开销将极大，一般还需要考虑KVCache的Offload策略，同时，因为涉及到CPU <--> GPU的数据传输，在工程设计上会更复杂，比如需要搭配异步调度策略去重叠召回开销等。

或者，可以考虑混合的异构KVCache管理策略，比如类似ShadowKV的思想，将K驻留到GPU，但把V卸载到CPU。

**2.从注意力内核中暴露中间逻辑(如Attention Score)**

稀疏KV与FA等注意力内核天然存在一些冲突，如：
1. FA的内核融合（fuse）隐藏了注意力权重：FA将 $QK^\top + online \;\; softmax + \times V$ 等操作都fuse到一个kernel中，中间的attention score不对外暴露；
2. FA的访存模式偏好连续、规则、密集，稀疏化往往带来 非连续、变长、跨块不规则等内存布局，破坏了FA的规则内存访问模式，可能降低算子利用率；
3. 全局排序 vs 在线softmax：许多稀疏策略（H2O等）需要一个能看到全局score的视角，而FA的在线 softmax逐块边计算边汇聚，没有天然的全局排序视角；
4. 位置编码（RoPE）的重排

解决思路：

结合PD分离 + 稀疏KV
1. Prefill全用 FA（密集，吞吐最佳）；
2. Prefill结束后，执行一次稀疏化，将连续化的稀疏KV传递到Decode；
3. Decode阶段仍可用FA（因输入再次连续化且大小更小），或者直接用Sparse Attention；

基于近似选取而非基于全局视角
1. 比如SnapKV仅基于输入最后一个窗口的AttentionScore去筛选Token，完全可以借助一个轻量的GEMM操作实现，而不必fuse到FA；
2. 亦或者GemFilter，用last-query的attention  score去挑选Token，无需完整attention matrix，也无需侵入FA内部；

借助小模型作为外置打分器(scorer)
1. 打分器用以对Token进行重要性评估，基于一个外置的小模型作为打分器，小模型执行标准Attention更快，并基于它获取AttentionScore等评估数据；

引入对稀疏KV更友好的注意力内核


**3.支持层粒度的非均匀内存布局**

很多KVCache稀疏算法会在不同的Layer或者Head上采取不同的稀疏策略，这要求vllm等推理引擎能够支持层粒度的非均匀内存布局。

在24年，vllm还不支持这种能力，因为它假设模型的所有层都采用Full Attention进行处理，然后在25年上半年，vllm已经陆续合入了混合内存管理器的特性，即：HybridKVCacheManager，它支持在模型的不同层采用不用的KVCache分配策略 该特性的落地为该解决该问题提供了保障。

![](imgs/Pasted%20image%2020250902140933.png)

如上架构，比如我们可以拓展一个新的Manager，如SparseKVManager，它负责在推理过程中，对模型的不同层分别进行KV稀疏，即通过淘汰策略将打分较低的KV进行删除，同时保留打分较高与距离较近的KV，从而节约内存并同时降低计算量与IO开销，最终实现推理加速。

一种最简单的策略是直接结合当前的FullAttentionManager与SlidingWindowManager，比如在模型的前几层使用Full Cache机制，即存储所有的KV，更高的层则通过Sliding Window自动替换打分最低的KV，通过这种方法就可以灵活的指定各个层所需要的内存空间，从而实现稀疏化。

同时，分析下现有的一些社区架构方向：

![](imgs/Pasted%20image%2020250907223929.png)

1. CachePolicy是一个管理KVCache分配的通用接口，它从基本的缓存行为（总是分配新的槽块来添加新的Token）推广到更复杂的缓存行为（规定在有新令牌时如何使用缓存空间），例如滑动窗口、H2O、FastGen等；
2. CachePolicy有两个方法：add_tokens_prefill和add_tokens_decode，用于将新的token分配给slot/block，给定的 slot/block 可能是新分配的，如果是，则会向分配器请求，也可能是旧的，来自之前分配的block，CachePolicy 负责选择不重要的Token，驱逐它们，并用新的token替换它们；

该RFC设计草案后期废弃，因为社区后续支持了混合内存管理器。

基于混合内存管理器架构，引入新的稀疏KVCache框架，架构采用如下可能会更好，该方案由近期的一个RFC提出草案。

![](imgs/Pasted%20image%2020250907223824.png)

首先，稀疏KVCache方法有很多，并且可能有不同的适应场景或方向，因此稀疏KVCache框架应该预先集成一部分稀疏算法，并支持动态选择或者基于参数指定，如：可以预先选择对H2O、SnapKV、Quest、NSA等提供支持。

SparseKVManager为不同的算法提供不同的KV块分配方法，为了将所有实现保留在SpareKVBase下，它将调用SparseKVBase，而真正的实现将在稀疏算法的子类中进行。

KVStoreBase有助于解耦稀疏算法和外部存储，它定义了与外部存储通信的方法，因此任何稀疏算法都可以与任何外部存储配合使用，这里的概念是通过ID和偏移量来标识块，这不仅适用于稀疏算法，也自然适用于前缀缓存，同时它也支持与KVStoreConnector适配以支持PD分离。

总结，一个逐步在生产级推理引擎中引入稀疏KV的走向：
1. 先针对H2O、SnapKV、Quest、StreamingLLM、Ada-KV等主流的、更易实现的稀疏算法进行复现，此阶段可以直接基于Transformer库，并利用已有开源实现，如论文开源Demo、LMCache开发分支、Nvidia/kvpress等进行复现，先摸透原理以及可能的踩坑点；
2. 针对vllm推理引擎，引入稀疏KVCache框架，可以先采用简单的实现，比如直接基于滑窗+Full Attention；
3. 跑通之后，再尝试丰富架构，支持更多特性，比如适配FA、适配更多稀疏算法、显存管理优化、适配PD分离等；

