论文地址：[eLLM: Elastic Memory Management Framework for Efficient LLM Serving](https://arxiv.org/html/2506.15155v1)

## 1.背景
1. LLM推理的显存由**静态权重**和**动态的激活与KV缓存**组成。现代系统（如vLLM）通过PagedAttention虚拟化KV缓存，几乎消除了KV空间的碎片，但激活仍由框架的静态张量抽象管理，两者分属不同抽象层，导致激活值空间与KVCache显存空间彼此隔离，无法在运行时相互借用显存；
2. 随着超长上下文和新架构（GQA/MLA/Jamba等）出现，内存构成正在变化：KV占比下降而激活占比上升，且prefill/decoding阶段激活占用波动巨大，传统方式会按最大长度预分配激活显存的做法将使大量激活显存长期闲置，从而导致KVCache侧资源紧张，造成吞吐损失可达约20%；

![550](imgs/Pasted%20image%2020250822063523.png)                                 
                                                                  (图1)
                                                                  
> [!Info] 
> 在单张NVIDIA A100（80GB）上启用LLaMA3-8B-262K（32并发，输入长度32768，输出2048）：
> （a）内存占用构成；
> （b）vLLM将激活与KV缓存隔离在不同空间分配，导致利用不足与次优性能；
> （c）eLLM基于动态内存分配，最大化利用率并获得约1.2倍加速；

如图a所示，请求长度的变化会显著影响张量的内存占用。与2K输入相比，262K输入的LLaMA-3-8B在prefill阶段使用了更大比例的激活张量，由于服务时请求长度不可预测性，现有框架（包括vLLM）会基于最大可能长度预分配激活，以避免动态分配开销，当请求较短（如< 8K tokens）时这个问题并不突出，但对长上下文推理的需求上升会导致GPU内存利用率显著不足；

其次，不同推理阶段也会导致激活值使用的大幅波动，例如，在LLaMA3-8B在262K输入下，从prefill转入decoding时，激活占比从40%+降至<10%，在单GPU上同时执行Prefill + Decode两阶段会导致内存利用不足，而最近提出的诸如PD分离等技术会将将prefill与decode拆分到不同GPU，可以降低每张卡的激活负担，但增加了资源占用与通信开销，同时仍采用静态激活分配与PagedAttention，因此也无法从根本上缓解内存问题；

此外，近期模型架构（如GQA、MLA、Jamba）聚焦于KV缓存压缩，旨在长上下文下减少内存压力，使模型能在更少GPU上服务，具体地，如图1(a)中显示，Jamba的KV缓存仅占19%，而激活从2K长度模型的0.3%跃升数十倍，这表明性能瓶颈已从**以KVCache为中心转向所有动态张量**；

从根本上看，**激活张量利用率低的根因在于运行时显存空间与KV缓存空间的隔离**，如图1(b)，三类张量被隔离，导致它们的内存空间之间出现不可逾越的鸿沟，最终降低内存利用效率与系统性能；

进一步的说明：

![650](imgs/Pasted%20image%2020250822070118.png)
                                                                  (图2)
- （a）早期LLM系统（如PyTorch）采用静态张量模型，无法处理动态KV扩张，导致碎片;
- （b）vLLM对KV缓存进行虚拟化，缓解碎片，但将激活与KV置于不同抽象层，进一步把激活与KV空间隔离;
- （c）eLLM在逻辑层独立管理KV与激活，但在物理层整合为统一内存池，最大化利用率;

早期LLM服务系统为张量分配固定大小、物理连续的内存块，并用池化重用策略管理。这对激活与参数有效，因为其张量大小在生命周期内不变，然而，KV缓存具有动态扩张、生命周期与序列长度不可预测等独特模式，传统内存管理不可避免导致严重碎片，如图2(a)；

vLLM通过引入PagedAttention，针对KV缓存进行虚拟化抽象，用页表机制消除了KVCache的物理连续要求，服务初始化时，依据模型的最大请求长度为运行时内存做预分配，其余内存组织为预配置的KV块，以容纳动态KV需求。通过提升GPU内存受限工作负载的内存效率，PagedAttention直接提高计算吞吐，成为主流系统的标准做法。

尽管现代系统（如vLLM）几乎无碎片地动态管理KV缓存空间，但要实现整体内存效率仍存在挑战，如图3。

![550](imgs/Pasted%20image%2020250822071113.png)
                                                                 (图3)
                                                                 
1. 随着模型上下文从数千扩至数十万乃至更高，GPU显存构成发生根本转变，如图3所示，在相同架构下，将上下文从2K扩至200K，激活空间占比从0.3%升至30.8%，而KV占比从89.6%降至59.1%；
2. 随着大模型的架构不断创新，GQA、MLA、Jamba等通过高效KV缓存技术缓解了长上下文内存压力，例如在200k上下文下，MPT-30B → Yi-34B → Jamba-52B，尽管参数量增加，激活:KVCache的比例随架构演进从0.52逆转到1.08；

eLLM论文提出，由**上下文扩张 × 架构创新共同驱动的GPU内存构成转变，将随时间继续加剧这一变化。**

## 2.方案与设计
![600](imgs/Pasted%20image%2020250822071945.png)
																 (图4)

如图4，揭示了当前系统设计里，显存未得到充分利用的几个特征。
1. **激活值空间未被有效利用**，以LLaMA3-8B且输入长度2K为例：prefill阶段，已分配的激活中仅35%在用，65%闲置，在真实数据集中，>90%的批次利用的上下文长度低于模型最大长度的30%，在decoding阶段，激活利用率进一步降至仅1%。与prefill一次处理更多token不同，decode每步只处理少量token，由于Decode的时间占比往往更大，这种极低的激活利用率主导了整体推理过程；
2. **KVCache空间过载**，历史KV积累：现代服务架构将历史KV与新请求共享在同一内存空间，随着上下文窗口扩展与前缀缓存采用，保留的KV条目显著增加，然而可用KV空间却反而减少，这种低效已成为现代LLM服务系统的重要瓶颈；

基于以上背景和发现，eLLM提出了一种弹性内存的管理机制：

Memory Ballooning是OS中一种放宽内存隔离、允许主机与虚机间动态再分配的经典机制，核心是基于虚拟内存的页表，通过映射关系传播实现再分配。eLLM受此启发，提出了：**一个能按实际需求在KVCache与激活值之间动态分配内存资源的弹性内存管理框架**。

- 在逻辑层面，eLLM继续将KV缓存与激活值抽象为不同逻辑实体，以便基于其访问模式的差异性采用专门策略，提高效率；
- 在物理层面，eLLM将所有物理内存统一为一个可在KVCache与激活值之间动态分配的共享池，减少碎片并最大化利用率，如图1(c)所示，激活与KV可彼此借用内存，在LLaMA-3-8B上带来了约20%的性能提升；

![600](imgs/Pasted%20image%2020250822073023.png)
                                                                   (图5)

如图5，eLLM由三大核心组件构成：虚拟张量抽象、弹性内存机制、轻量级调度策略。

首先，eLLM为KV缓存与激活提出虚拟化张量抽象，反映各自的访问特性，将其与物理资源解耦，为弹性内存管理奠定基础；随后，基于该抽象，eLLM引入弹性内存机制，通过重映射动态重排KVCache与激活空间，并将CPU内存作为弹性缓冲进一步缓解GPU压力，最后，eLLM利用弹性机制设计轻量级调度策略，以在SLO约束下高效利用内存并实现有效权衡。

### 2.1 虚拟张量抽象
eLLM引入面向LLM负载的新张量抽象eTensor，利用GPU虚拟内存管理（VMM）弥合现有系统中激活与KV之间的抽象鸿沟。从内核函数视角，eTensor可视为引用GPU虚拟地址空间中一段连续区域的数组指针结构，在虚拟地址层之下，VMM维护完整的物理块映射，使计算核能访问存于全局GPU内存的所需数据，这使映射关系可在不同张量间动态传播。

鉴于KVCache与激活在分配粒度与使用模式上的差异，eTensor引入两种张量类型：KV eTensor与Activation eTensor，并为各自配备专业策略以优化利用与效率（如图6）。
![600](imgs/Pasted%20image%2020250822073342.png)
															  (图6)

- KV缓存天然呈大而规则的内存块，扩张稳定、访问不频繁、在推理期间持久保留。因此，在KV eTensor中，为每个请求在最大并发下预留与模型上下文长度等长的一段虚拟地址，确保KV在逻辑上的连续性；物理块在实际写入时按需分配，避免不必要的物理占用；
- 激活值由较小内存块构成，寿命短、访问频繁。因此，Activation eTensor的虚拟段大小不均，天然需要频繁且细粒度地管理虚拟地址空间；

两类eTensor的虚拟段（称为tensor slots，即张量槽位）都与物理块粒度严格对齐，以在访问效率与碎片控制之间取得均衡。

鉴于虚拟化引入的虚实地址映射开销，eLLM为每类eTensor实现定制的内存池管理。
1. 生命周期结束时，并非立即将eTensor与物理资源解绑，而是将其标记为：已映射、可用的张量槽位，并记录映射大小，以便高效复用；
2. KV eTensor池对新请求采用“最佳适配（Best-Fit）”：在已映射/可用集合R中，针对目标大小s，选择满足大小≥s的最小槽位；若不存在，则触发按需映射；
3. Activation eTensor池沿用框架原生的BFC（Best-Fit with Coalescing）策略；

由于eTensor分池管理，所有物理块都带有其对应eTensor类别的标签（称为“所有权”），但这些物理块实质上属于统一的物理内存池，依靠“映射关系传播”，我们仅通过标识转换即可实现零开销的归属变更，从而允许eLLM依据运行时负载动态分配资源。

### 2.2 弹性内存机制
弹性内存机制旨在在LLM负载下提升内存利用与系统性能，它引入两级弹性：
- GPU内的膨胀/收缩，使激活与KV的物理占用随动态负载及时调整；
- GPU-CPU间的卸载/回拉，将CPU用作弹性缓冲，进一步缓解GPU压力；

膨胀/收缩是eLLM的核心机制，通过“映射关系传播”打破内存隔离，本质是动态维护虚拟地址空间与物理块之间的绑定关系，构建在eTensor抽象之上。

膨胀操作通过向激活池借用物理块来扩展KV缓存的物理容量，类似“给气球充气”。

流程：
① 触发：当KV分配请求到达，若KV池中物理块不足，则向激活池发起借用请求；
② 回收：激活池触发轻量GC，识别并解除映射那些分配给“非活跃eTensor对象”的物理块；
③ 所有权转移：回收的块通过所有权转移，从激活池逻辑迁移至KV池；
④ 按需重映射：GPU VMM将这些块映射到目标KV eTensor的虚拟地址空间；
收缩为上述逆过程。

图7通过例子展示了GPU内的弹性机制：
（a）初始GPU保留历史KV；在现有系统中，由于KV空间不足，新请求无法立即处理；
（b）通过膨胀，eLLM可借用闲置激活块以立即处理prefill请求；
（c）通过膨胀，eLLM在高内存压力下仍能扩大批量，提高decode阶段这种“内存受限环节”的资源利用；
（d）通过收缩，eLLM可归还借用的内存；实践中该过程延迟触发以避免不必要开销；

![700](imgs/Pasted%20image%2020250822074515.png)
															(图7)

eLLM进一步在GPU-CPU间引入弹性管理，以在高内存压力下降低服务响应压力，在线长上下文服务中，优化TTFT比优化TPOT更具挑战：
（1）更多内存争用，导致严重排队延迟；
（2）prefill阶段计算复杂度是超线性的；

因此，eLLM在prefill阶段主动将部分请求的KV缓存卸载到CPU DRAM，降低请求执行的“内存准入门槛”，有效减少排队、改善TTFT；同时有望汇聚更大的解码批量以提升吞吐。

其技术可行性基于：
第一，新增但“短期不需”的KV可主动卸载，仅在对应请求的decode被调度时回拉；
第二，Transformer的多层结构天然支持按层流水，重叠计算与通信；
第三，KV迁移通信开销为O(N)，而prefill自注意力计算复杂度为O(N^2)，在A100+LLaMA-3-8B等实测中，卸载开销可被计算完全掩蔽，不过，CPU缓冲改善排队条件，天生偏向prefill，可能不利于TPOT，因此eLLM引入简单有效的SLO感知策略，在二者间权衡。

### 2.3 轻量级调度策略
eLLM引入了一种轻量但有效的调度算法，利用弹性内存机制动态分配资源并优化性能，基础调度策略采用简单启发式，仅依赖元数据更新与二元决策，在受限内存下最大化利用率，进一步地，通过一个简洁的SLO感知缓冲缩放策略，eLLM在TTFT与TPOT间取得平衡，保障在线服务的SLO达成。

具体实现略，本质是基于时间窗口连续检测TTFT、TPOT，然后调整缓冲区，策略较为简单。

## 总结

**1）背景与问题**
- LLM显存构成：权重（静态）+ 激活/KV缓存（动态），长上下文与新架构让激活占比上升、KV占比下降，且在prefill高、decode低，波动大；
- 现状痛点：vLLM仅虚拟化 KV（PagedAttention），激活仍走框架静态分配，二者隔离，无法在运行时互相“借用”显存，导致KV紧张、激活闲置，吞吐与批量受限；

**2）eLLM的方案与实现**
- 统一物理池：用虚拟张量eTensor 将“虚拟地址”与“物理显存块”解耦；KV/激活各有专用eTensor与池化策略，但物理块共享；
- 弹性内存：
    - GPU内“膨胀/收缩”：KV不足→向激活借块；阶段变化时动态再平衡；
    - GPU-CPU“卸载/回拉”：prefill将短期不用的KV卸 CPU，decode再回拉，重叠通信与计算；
- 轻量调度+SLO感知；

**3）可行性、价值、工作量评估**
- 价值（论文实测）：TTFT最高降至2.95×，Decode吞吐最高2.32×，长上下文可支持更大批次（至多3×）；
- 具备可行性
- 引入工作量评估（中等）：
	  - eTensor与统一物理池：接入GPU VMM，重构引擎侧的KVCache与激活的内存管理逻辑，实现物理块的所有权机制（基于Rust实现是否更好？）；
	  - 膨胀/收缩机制：水位监控、轻量GC、重映射时机控制等实现；
	  - GPU-CPU通道：KV卸载/回拉的异步流水与失败回退；（可集成于现有的KVCache卸载系统）
	  - 调度与SLO（这个视情况可不实现或进一步优化）
	  - 工程性调优与优化；
