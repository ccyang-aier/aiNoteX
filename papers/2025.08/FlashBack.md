论文地址：[FlashBack:Efficient Retrieval-Augmented Language Modeling for Fast Inference](https://arxiv.org/html/2405.04065v4)
代码仓：[FlashBack](https://github.com/BIT-NLP-GROUP/FlashBack)


本论文的提出基于对RAG场景的一个明显问题：长输入上下文带来的昂贵计算开销；

- **问题**：检索增强生成（RAG/RALM）很有用，但“把检索结果放在输入最前面”的常见做法会让推理变慢，尤其是长上下文与大模型。
- **核心点**：把检索结果“放在末尾”（后置 append），这样可以重复利用注意力的 KV 缓存，不用反复重算，推理更快。
- **问题是**：后置会让模型不太“习惯”，性能（PPL）会掉。
- **解决**：在后置的上下文里，给检索片段加一对“标记 Token”，然后用少量参数（LoRA）做个小微调，让模型学会在后置结构下读懂检索内容。
- **结果**：推理更快（最多约 4×），性能几乎追平前置。


![1000](imgs/Pasted%20image%2020250824234348.png)

![](imgs/Pasted%20image%2020250824234429.png)


## 为什么“前置检索”会慢？

Transformer 推理会缓存每一步的 Key/Value（KV cache）。

- **前置模式（prepend）**：每次把新检索段插在最前面，所有后面的 token 相对位置都变了，于是原来的 KV 缓存几乎全作废，要重算一大片。
- **后置模式（append）**：把新检索段接在末尾，前面输入的位置都没变，之前的 KV 缓存就能复用，只需算新增的末尾那一段。


为什么后置会影响效果？

模型原来被“教会”的是：重要的检索内容往往在前面（或者前后关系有一定“语义结构”）。你现在把它放到后面，模型就需要“适应”这种结构变化，否则它不一定会把那段信息当回事，于是困惑度（PPL）会变差。


## 怎么补救：标记 Token + 微调（LoRA）

- 在每段检索内容的左右各加一个特殊符号（比如 `<MARK_L>` 和 `<MARK_R>`），明确告诉模型：这是一段“检索证据”，请重点关注。
- 模型主体不动，只用 **LoRA** 微调注意力中的少量参数，再把这两个“标记 Token”的嵌入学出来。
- 好处：
    - 训练代价低（参数少、时间短、单卡可跑）。
    - 不破坏原模型能力（比全参微调“遗忘”更少）。
    - 让模型适应“后置结构”，把效果拉回去。


## 算法流程

- 初始化：有一段输入文本，模型每生成一段 token（比如每 16 个）就进行一次检索。
- 每次检索：
    - 用当前前缀（固定长度，比如最近的 16 或 32 个 token）去外部知识库里搜文档片段。
    - 把检索到的片段用“标记 Token”包起来：`<MARK_L> ...证据... <MARK_R>`。
    - 把这段证据“放在上下文末尾”（append），继续生成。
- 因为都是往后加，之前输入的 KV 缓存可以复用，速度快。


## 性能对比

- **速度**：在 Llama 2 7B 上，后置方案在长上下文和多次检索条件下能达到大约 4× 加速（相对于前置）。
- **语言建模（PPL）**：
    - 直接后置不微调，PPL 会变差。
    - 后置 + “标记 Token + LoRA”微调后，PPL 能显著改善，逼近前置。模型越大越接近。

## 关键可调参数与实践建议

- **检索步长 (s)**：每隔多少个生成 token 检索一次。
    - 论文发现把 (s) 从 16 增加到 32 或 64，PPL 不一定变差（有时更好），但速度会更快，因为检索更少、重算更少。
    - 实战里可以先用 16 起步，再逐渐加大，观察精度与速度的平衡。
- **查询长度 **：向检索器提供多少个最近 token 作为查询（如 16 或 32）。
    - 太短可能检不到相关文档，太长会增加检索系统负担。
    - 通常从 16/32 做经验起点。
- **检索文档数（top-k）**：
    - 论文发现 top-2 往往已经最好；
    - 再多可能引入噪声，还会拖慢推理（尤其是要把每个文档都拼进去或做概率融合时）。
- **标记 Token 的使用**：
    - 一定要在检索片段左右包上标记；
    - 微调时也保持一致的数据格式，模型才会学会“看到标记=关注检索内容”。
- **LoRA 设置**：
    - 只在注意力的 Key/Value 投影上加 LoRA，秩 (r) 可设 16；
    - 主体模型权重冻结；
    - 5k 步级别的小规模训练通常就够用。
- **检索器选择**：
    - 简单 BM25 就能跑；
    - 需要更好 QA 精度时，换 DPR/向量检索；
    - 方法对检索器是“解耦”的。

## 什么时候用 FlashBack 最划算？
- 你的任务是“长上下文 + 持续检索 + 逐步生成”，例如：
    - 长篇问答/写作辅助，边写边查；
    - 技术支持/日志分析，持续把最新信息拼到上下文里；
    - 交互式代理，反复外部检索并整合。
- 你的模型比较大（B 级参数），或上下文很长（几千 token），前置带来的 KV 重算开销极大。

如果你的任务一次性把文档都准备好、很少追加，前置/后置差异就没这么大；那你可以更关注检索质量本身。



## 最小可行实现

- 数据准备：
    - 给训练样本构造“连续检索”的场景：每隔 (s) 个 token，取最近 (\ell) 个 token 去检索，取回 top-1/2 片段。
    - 在检索片段两侧加 `<MARK_L>` 和 `<MARK_R>`，把它们放在末尾再继续生成。
- 训练：
    - 冻结 LLM 主干参数；
    - 仅训练：两枚标记 Token 的嵌入 + 注意力 KV 投影处的 LoRA（秩 16 起）；
    - 训练 5k 步量级，cosine 学习率调度，batch size 视显存而定。
- 推理：
    - 每生成 (s) 个 token 再检索一次；
    - 新证据用标记包裹并追加到末尾；
    - 复用之前的 KV 缓存，只算新增段；
    - 控制 top-k=2，避免堆太多文档。


