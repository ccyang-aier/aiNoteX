论文地址：[Parametric Retrieval Augmented Generation](https://arxiv.org/html/2501.15915v1)
代码仓地址：[PRAG](https://github.com/oneal2000/PRAG)
## 1.背景
如图1，传统RAG将检索到的相关文档的token与查询一起拼接作为输入，使用原始LLM参数θ来回答问题，不修改模型参数；PRAG则基于检索到的文档更新LLM参数θ′= θ + Δθ，临时将相关知识整合到LLM的参数中以回答问题。

![1000](imgs/Pasted%20image%2020250824180217.png)
															(图1)

PRAG的提出和理论基于以下背景：

1. 一旦训练完成，LLM的内部知识基本静态化，难以纳入新近出现的信息或预训练数据中未包含的知识，为了应对这一限制，RAG成为主流的重要解决方案，它通过从外部语料中检索相关信息，使LLM能动态访问和利用其预训练参数之外的知识，从而提升模型的适应性与性能；
2. 现有研究从多方面探索了RAG流水线及其优化，例如检索时机、文档选择和外部知识组织等，尽管这些创新能在流程的不同阶段带来改进，但所有的RAG方法（无论变体如何）都有一个共同点：通过将相关段落或文档直接加入LLM的输入上下文来注入外部知识，即：上下文内知识注入；
3. 上下文注入的方式简单且有效，但近来的研究指出它存在以下缺陷：
   - 不可避免地增加了上下文长度，长上下文不仅带来额外计算开销与推理时延，还会削弱LLM对外部知识的理解与利用，尤其在复杂推理任务中；
   - LLM在上下文中处理信息的方式，与其利用参数中存储的内部知识的方式在本质不同。LLM的大部分知识存储在其神经网络结构的参数中（如FFN层参数），把段落或文档添加到输入上下文，本质上只影响注意力网络的在线KV计算，而不改变模型自身的参数，这意味着LLM无法像利用内部知识那样高效的使用RAG检索到的外部知识；

基于以上背景，PRAG被提出，它将外部知识直接注入LLM的前馈网络（FFN）：
1. 具体来说，PRAG首先进行离线预处理，将外部语料中的每篇文档参数化，转换为一小组参数（通常每文档几MB），可直接整合进下游LLM，这一组参数称为该文档的“参数化表示”；
2. 在推理阶段，PRAG采用“检索-更新-生成（RUG）”流程，如图1所示，检索步骤与传统RAG相同：根据输入提示从外部语料检索top-n文档，随后在“更新”步骤中，使用检索到文档的参数化表示来更新LLM，最后在“生成”步骤，使用更新后的LLM基于原始输入提示直接推理；

## 2.方案与设计
首先给出当前方案的问题建模概述：

设有LLM：ℒ，基础参数为：θ，给定用户查询：q，我们希望借助外部语料：K生成准确回答，K定义为$K=\{d_1,d_2,…,d_N\}$，其中每个$d_i$代表一个文本块.

系统包含检索模块：R，它针对查询q计算每个文档的相关性分数：$\{S_{d_1},S_{d_2},…,S_{d_N}\}$，传统RAG选择相关性最高的前k篇文档作为外部知识，并追加到ℒ的输入上下文中，通常配合一个提示模板，指示ℒ基于所给知识生成回答。

不同于将文档注入输入的上下文RAG，PRAG直接将文档插入到ℒ的参数中，为此，PRAG包含两个阶段：**离线文档参数化**与**在线“检索-更新-生成”推理**。

### 2.1 文档离线参数化

![1000](imgs/Pasted%20image%2020250824200226.png)
                                                        (图2)
如图2，文档离线参数化过程分为两个主要阶段：

- **阶段一：Document Augmentation（文档增强）** - 通过改写和QA生成丰富文档表示
- **阶段二：Parametric Document Encoding（参数化文档编码）** - 将增强后的文档训练为LoRA参数

#### 2.1.1 Document Augmentation（文档增强）

##### 1.文档改写步骤（Document Rewriting）

**目标**：为每个原始文档生成多样化的语言表达，保持事实内容不变但增加表述多样性。

**具体操作**：
1. 对于语料库 $K$ 中的每篇文档 $d_i$
2. 使用预设提示词指导LLM进行文档改写
3. 生成$n$个不同版本的改写文档：${d_i^1, d_i^2, \ldots, d_i^n}$

**改写原则**：
- 保持原文档的核心事实和信息不变
- 采用不同的措辞、句式结构和表达风格
- 确保改写后的文档在语义上与原文档等价

**数学表示**： $$\text{Rewrite}: d_i \rightarrow {d_i^1, d_i^2, \ldots, d_i^n}$$
##### 2.问答对生成步骤（QA Pair Generation）

**目标**：基于原始文档生成相关的问答对，强化模型对文档事实的理解和应用能力。

**具体操作**：
1. 以原始文档$d_i$为输入
2. 使用专门设计的提示词指导LLM生成问题
3. 针对每个生成的问题，基于文档内容生成对应答案
4. 产生$m$组问答对：${(q_i^1, a_i^1), (q_i^2, a_i^2), \ldots, (q_i^m, a_i^m)}$

**QA生成策略**：
- 问题应覆盖文档中的关键事实和概念
- 答案必须基于文档内容，避免外部知识引入
- 问答对应具有多样性，涵盖不同类型的信息提取

**数学表示**： $$\text{QA Generation}: d_i \rightarrow {(q_i^1, a_i^1), (q_i^2, a_i^2), \ldots, (q_i^m, a_i^m)}$$

##### 3.增强数据集构建

**目标**：将改写文档和问答对组合成完整的训练数据集。

**组合规则**： 对每篇文档$d_i$，构建增强数据集$D_i$：

$$D_i = {(d_i^k, q_i^j, a_i^j) \mid 1 \leq k \leq n, 1 \leq j \leq m}$$
其中：
- $d_i^k$ 表示第 $k$ 个改写版本
- $(q_i^j, a_i^j)$ 表示第 $j$ 个问答对
- 每个三元组 $(d_i^k, q_i^j, a_i^j)$ 构成一个训练样本

#### 2.1.2 Parametric Document Encoding（参数化文档编码）

##### 1.LoRA参数初始化

**目标**：为每篇文档初始化对应的低秩矩阵参数。

**理论基础**：基于LoRA（Low-Rank Adaptation）方法，对LLM的FFN权重矩阵进行低秩分解：
$$W' = W + \Delta W = W + AB^T$$

其中：
- $W \in \mathbb{R}^{h \times k}$：原始FFN权重矩阵（冻结）
- $A \in \mathbb{R}^{h \times r}$：低秩矩阵A（可训练）
- $B \in \mathbb{R}^{k \times r}$：低秩矩阵B（可训练）
- $r \ll \min(h, k)$：秩的维度，远小于原矩阵维度

**参数设置**：
- $h$：LLM隐藏层维度
- $k$：FFN中间层维度
- $r$：低秩维度（通常设为2-8）
- $\Delta\theta = {A, B}$：每个文档的参数化表示

##### 2.训练序列构建

**目标**：将增强数据转换为适合语言模型训练的token序列。

**序列拼接**：对于增强数据集 $D_i$ 中的每个三元组 $(d_i^k, q_i^j, a_i^j)$，构建训练序列：
$$x = [d_i^k \oplus q_i^j \oplus a_i^j]$$
其中：
- $\oplus$ 表示token序列的拼接操作
- $x$ 为完整的训练序列
- 序列长度为 $T = |d_i^k| + |q_i^j| + |a_i^j|$

##### 3.损失函数优化

**目标**：通过标准的下一token预测任务训练文档特定的LoRA参数。

**优化目标**： $$\min_{\Delta\theta} \sum_{(d_i^k, q_i^j, a_i^j) \in D_i} \sum_{t=1}^{T} -\log P_{\theta + \Delta\theta}(x_t | x_{<t})$$

**公式解释**：
- $\theta$：预训练LLM的基础参数（冻结不变）
- $\Delta\theta = {A, B}$：当前文档的LoRA参数（可训练）
- $P_{\theta + \Delta\theta}(x_t | x_{<t})$：在位置 $t$ 预测token $x_t$ 的概率
- $x_{<t}$：位置 $t$ 之前的所有token
- 外层求和遍历所有训练三元组
- 内层求和遍历序列中所有token位置

**训练策略**：
- 对序列中的每个token（包括文档、问题和答案token）都计算损失
- 这确保模型在训练过程中内化文档的所有信息
- 避免仅在答案部分计算损失的局限性

##### 4.参数保存与管理

**目标**：将训练完成的LoRA参数作为文档的参数化表示进行存储。

**存储格式**：每个文档 $d_i$ 对应的参数化表示： $$p_i = {A_i, B_i}$$
**参数规模估算**：
- 每个文档的参数量：$2nr(h + l)$
- $n$：Transformer层数
- $r$：LoRA秩
- $h$：隐藏维度
- $l$：FFN中间维度

**示例计算**（LLaMA3-8B，$r=2$）： $$\text{参数量} = 2 \times 32 \times 2 \times (4096 + 14336) = 2,359,296 \text{ 参数}$$ $$\text{存储空间} = 2,359,296 \times 2 \text{ bytes} \approx 4.72 \text{ MB}$$

#### 2.1.3 公式推导总结

1. **LoRA权重更新**： $$W' = W + AB^T$$
    
2. **增强数据集定义**： $$D_i = {(d_i^k, q_i^j, a_i^j) \mid 1 \leq k \leq n, 1 \leq j \leq m}$$
    
3. **训练序列构建**： $$x = [d_i^k \oplus q_i^j \oplus a_i^j]$$
    
4. **损失函数**： $$\mathcal{L} = \sum_{(d_i^k, q_i^j, a_i^j) \in D_i} \sum_{t=1}^{T} -\log P_{\theta + \Delta\theta}(x_t | x_{<t})$$
    
5. **参数化表示映射**： $$f_\phi: d_i \rightarrow p_i = {A_i, B_i}$$
    



为什么需要文档增强？
1. **语言多样性**：改写提供不同表达方式，增强模型的语义理解
2. **应用导向**：QA对训练模型以问答形式应用知识，而非仅仅记忆文本
3. **知识内化**：多样化训练有助于将事实知识转化为可应用的参数化表示

为什么选择LoRA？
1. **参数效率**：低秩分解大幅减少需要训练和存储的参数量
2. **模块化**：不同文档的LoRA参数可以独立训练和合并
3. **灵活性**：推理时可以动态加载和卸载不同文档的参数

通过上述详细的步骤说明，我们可以看出文档离线参数化过程是一个系统性的知识转换流程，将非结构化的文本文档转换为结构化的参数表示，为后续的在线推理阶段奠定基础。

### 2.2 在线推理

#### 2.2.1 检索阶段（Retrieval）

##### 1.输入处理
- **输入**：用户查询 $q$
- **目标**：从预处理好的外部语料库 $K={d_1, d_2, ..., d_N}$ 中找到最相关的文档

**相关性计算**

使用检索器 $R$ 计算每个文档 $d_i \in K$ 相对于查询 $q$ 的相关性分数：

$$S_{d_i} = R(q, d_i)$$

其中：
- $S_{d_i}$ 表示文档 $d_i$ 的相关性分数
- $R$ 可以是BM25、DPR等检索模型

##### 2.文档选择

根据相关性分数，选择前 $k$ 篇最相关的文档：

$${d_1, d_2, ..., d_k} = \text{TopK}({S_{d_i}}_{i=1}^{N})$$

##### 3.参数化表示获取

对于每个被检索到的文档 $d_i$，获取其对应的预训练好的参数化表示：

- 低秩矩阵对：$(A_i, B_i)$
- 其中 $A_i \in \mathbb{R}^{h \times r}$，$B_i \in \mathbb{R}^{k \times r}$
- $h$ 是隐藏维度，$k$ 是FFN中间维度，$r$ 是LoRA的秩（$r \ll \min(h, k)$）

#### 2.2.2 更新阶段（Update）

##### 1.参数合并

将检索到的 $k$ 篇文档的低秩矩阵进行合并，形成单一的权重更新：

$$\Delta W_{merge} = \alpha \cdot \sum_{j=1}^{k} A_j B_j^{\top}$$

**公式说明**：
- $\alpha$ 是标量缩放因子，用于调节更新幅度
- $A_j B_j^{\top}$ 是第 $j$ 篇文档对应的低秩权重更新
- 求和操作将多篇文档的知识聚合为一个统一的参数更新

##### 2.FFN权重更新

对于LLM中每个Transformer层的FFN权重矩阵 $W \in \mathbb{R}^{h \times k}$，应用合并后的参数更新：

$$W' = W + \Delta W_{merge}$$

**更新说明**：

- $W$ 是原始预训练的FFN权重（保持冻结）
- $W'$ 是临时更新后的权重，包含了检索文档的知识
- 这个更新过程在所有Transformer层的FFN中同时进行

##### 3.临时模型构建

通过权重更新，得到临时的增强模型：

$$\mathcal{L}'(\theta') = \mathcal{L}(\theta + \Delta\theta)$$

其中：

- $\mathcal{L}$ 是原始LLM
- $\theta$ 是原始模型参数
- $\Delta\theta = {\Delta W_{merge}}$ 表示所有层的参数更新集合
- $\mathcal{L}'(\theta')$ 是注入外部知识后的临时模型

#### 2.2.3 生成阶段（Generation）

##### 1.直接推理

使用更新后的模型 $\mathcal{L}'(\theta')$ 对原始查询 $q$ 进行推理，**无需**在输入中添加任何检索到的文档文本。

##### 2.自回归解码

采用标准的从左到右解码过程：

$$P(y_t | y_{<t}, q) = \text{softmax}(\mathcal{L}'(\theta')[y_{<t}, q])$$

其中：

- $y_t$ 是第 $t$ 个生成的token
- $y_{<t}$ 是前 $t-1$ 个已生成的token
- 模型基于更新后的参数进行概率计算

##### 3.答案生成

继续解码直到满足停止条件（如遇到结束符或达到最大长度），得到最终答案：
$$\text{Answer} = {y_1, y_2, ..., y_T}$$

#### 2.2.4 效率优化与资源管理

##### 1.内存管理

- **LoRA加载时间**：约占单个token解码时间的1%
- **参数更新**：仅影响FFN层，不修改注意力权重

##### 2.计算复杂度

在线推理的时间复杂度为：

$$\mathcal{O}(|q|^2 h + |q| h^2)$$

相比传统上下文RAG的复杂度：

$$\mathcal{O}((t|d| + |q|)^2 h + (t|d| + |q|) h^2)$$

**复杂度优势**：

- 节省了 $\mathcal{O}(t^2|d|^2 h + t|d||q|h + t|d|h^2)$ 的计算
- 其中 $t$ 是检索文档数，$|d|$ 是平均文档长度

##### 3.推理完成后处理

- **参数恢复**：推理完成后，模型参数自动恢复到原始状态 $\theta$
- **内存释放**：临时的LoRA参数可被释放，为下一次查询做准备
- **并发支持**：不同查询可以独立进行参数更新和推理

关键优势总结
1. **计算效率**：避免了长上下文的二次复杂度
2. **参数一致性**：外部知识直接注入参数空间，与内部知识处理方式一致
3. **灵活性**：可根据需要动态加载不同数量的文档参数
4. **兼容性**：可与传统上下文RAG方法结合使用

通过这种"检索-更新-生成"的流程，Parametric RAG实现了高效的参数化知识注入，在保持推理效率的同时显著提升了知识增强效果。