论文地址：[Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks](https://arxiv.org/html/2412.15605v2)
代码仓地址：[CAG](https://github.com/hhhuang/CAG)
## 1.背景
检索增强生成（RAG）通过动态集成外部知识源，显著提升了大语言模型的能力，RAG系统已在处理开放域问题与专业任务方面卓有成效，基于检索提供与上下文相关性的答案。然而，RAG并非完美：

- 实时检索带来延迟；
- 相关文档的选择与排序错误会降低生成质量；
- 将检索与生成组件整合提高了系统复杂度，需要精细调参并增加维护开销；

![600](imgs/Pasted%20image%2020250824171756.png)
                                                                  (图1)
如图：在RAG流程中，IR模型从知识源检索相关信息，检索到的知识和查询都会在推理过程中由LLM处理，从而引入检索延迟。

本文提出一种替代范式——缓存增强生成（CAG），与RAG基于检索的方法不同，我们将所有相关文档提前预加载到LLM，并预计算键值缓存，通过预加载上下文，模型在运行时无需额外检索即可提供丰富且语境准确的答案。这种方法消除了检索延迟，缓解检索错误，简化系统架构，同时通过让模型整体处理全部相关上下文来保持高质量的响应。

## 2.方案与设计
CAG的流程分为三部分：
1. 外部数据源预加载，计算KVCache，持久化到内存或磁盘；
2. 推理时，加载这些KVCache，并于用户查询q一同参与推理过程；
3. 缓存重置，驻留内存或磁盘的KVCache可实时重置，保证知识源的实时性和不被污染；

## 3.总结
1. 不算是一篇算法创新或者是技术创新等方面的论文，基本没有参考价值；







